{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7705 images belonging to 2 classes.\n",
      "Found 121 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 467s 5s/step - loss: 0.6110 - acc: 0.7247 - val_loss: 3.4073 - val_acc: 0.3140\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.31405, saving model to D:\\Python\\cv\\keras_model\\my_model-01epoch-0.31acc.hdf5\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 482s 5s/step - loss: 0.4679 - acc: 0.7895 - val_loss: 9.5263 - val_acc: 0.2479\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.31405\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 458s 5s/step - loss: 0.3757 - acc: 0.8394 - val_loss: 10.4560 - val_acc: 0.2479\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.31405\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.3114 - acc: 0.8733 - val_loss: 10.2472 - val_acc: 0.2479\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.31405\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 453s 5s/step - loss: 0.2856 - acc: 0.8848 - val_loss: 3.9105 - val_acc: 0.2562\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.31405\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 457s 5s/step - loss: 0.2580 - acc: 0.9005 - val_loss: 2.2430 - val_acc: 0.4545\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.31405 to 0.45455, saving model to D:\\Python\\cv\\keras_model\\my_model-06epoch-0.45acc.hdf5\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 452s 5s/step - loss: 0.2332 - acc: 0.9050 - val_loss: 0.3772 - val_acc: 0.8512\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.45455 to 0.85124, saving model to D:\\Python\\cv\\keras_model\\my_model-07epoch-0.85acc.hdf5\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 480s 5s/step - loss: 0.2498 - acc: 0.8967 - val_loss: 6.6720 - val_acc: 0.2479\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.85124\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 481s 5s/step - loss: 0.2201 - acc: 0.9119 - val_loss: 1.1154 - val_acc: 0.6446\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.85124\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 449s 4s/step - loss: 0.2221 - acc: 0.9132 - val_loss: 0.2638 - val_acc: 0.9421\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.85124 to 0.94215, saving model to D:\\Python\\cv\\keras_model\\my_model-10epoch-0.94acc.hdf5\n",
      "Wall time: 1h 17min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from PIL import Image,ImageFile\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalAveragePooling2D,Dropout,BatchNormalization,Conv2D,MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "size=224\n",
    "train=ImageDataGenerator(rescale=1./255,\n",
    "                                 horizontal_flip=True,\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2)\n",
    "val=ImageDataGenerator(rescale=1./255)\n",
    "train_generator=train.flow_from_directory(directory='d:\\Python\\cv\\\\model_data',\n",
    "                                                  target_size=(size,size),\n",
    "                                                  batch_size=64,\n",
    "                                                  class_mode='categorical')\n",
    "val_generator=val.flow_from_directory(directory='d:\\Python\\cv\\\\test_data',\n",
    "                                                target_size=(size,size),\n",
    "                                                batch_size=64,\n",
    "                                                class_mode='categorical')\n",
    "model=Sequential()\n",
    "model.add(Conv2D(input_shape=(size,size,3),kernel_size=(3, 3), filters=6, padding=\"same\", kernel_initializer=\"uniform\", use_bias=False))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(kernel_size=(5, 5), filters=16, padding=\"same\", kernel_initializer=\"uniform\", use_bias=False))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(kernel_size=(5, 5), filters=64, padding=\"same\", kernel_initializer=\"uniform\", use_bias=False))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(120, activation='relu'))\n",
    "model.add(Dropout(0.35))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(120, activation='relu'))\n",
    "model.add(Dropout(0.35))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "path='D:\\Python\\\\cv\\\\keras_model\\\\my_model-{epoch:02d}epoch-{val_acc:.2f}acc.hdf5'\n",
    "checkpointer=ModelCheckpoint(path,monitor='val_acc',save_best_only=True,verbose=1)\n",
    "\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=100,\n",
    "                   epochs=10,\n",
    "                   validation_data=val_generator,\n",
    "                   validation_steps=2,\n",
    "                   callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7870 images belonging to 2 classes.\n",
      "Found 249 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      "120/120 [==============================] - 763s 6s/step - loss: 0.1989 - acc: 0.9259 - val_loss: 2.4110 - val_acc: 0.5781\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.57812, saving model to D:\\Python\\cv\\keras_model\\my_model-01epoch-0.58acc.hdf5\n",
      "Epoch 2/30\n",
      "120/120 [==============================] - 726s 6s/step - loss: 0.1746 - acc: 0.9344 - val_loss: 0.4991 - val_acc: 0.8649\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.57812 to 0.86486, saving model to D:\\Python\\cv\\keras_model\\my_model-02epoch-0.86acc.hdf5\n",
      "Epoch 3/30\n",
      "120/120 [==============================] - 723s 6s/step - loss: 0.1738 - acc: 0.9311 - val_loss: 0.6653 - val_acc: 0.7243\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.86486\n",
      "Epoch 4/30\n",
      "120/120 [==============================] - 689s 6s/step - loss: 0.1611 - acc: 0.9392 - val_loss: 1.4044 - val_acc: 0.7189\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.86486\n",
      "Epoch 5/30\n",
      "120/120 [==============================] - 687s 6s/step - loss: 0.1624 - acc: 0.9376 - val_loss: 0.7978 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.86486\n",
      "Epoch 6/30\n",
      "120/120 [==============================] - 725s 6s/step - loss: 0.1587 - acc: 0.9393 - val_loss: 0.3657 - val_acc: 0.8811\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.86486 to 0.88108, saving model to D:\\Python\\cv\\keras_model\\my_model-06epoch-0.88acc.hdf5\n",
      "Epoch 7/30\n",
      "120/120 [==============================] - 693s 6s/step - loss: 0.1572 - acc: 0.9402 - val_loss: 1.7142 - val_acc: 0.5676\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.88108\n",
      "Epoch 8/30\n",
      "120/120 [==============================] - 738s 6s/step - loss: 0.1456 - acc: 0.9460 - val_loss: 1.9292 - val_acc: 0.6162\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.88108\n",
      "Epoch 9/30\n",
      "120/120 [==============================] - 724s 6s/step - loss: 0.1523 - acc: 0.9411 - val_loss: 0.6713 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.88108\n",
      "Epoch 10/30\n",
      "120/120 [==============================] - 712s 6s/step - loss: 0.1495 - acc: 0.9456 - val_loss: 1.0026 - val_acc: 0.7622\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.88108\n",
      "Epoch 11/30\n",
      "120/120 [==============================] - 796s 7s/step - loss: 0.1425 - acc: 0.9500 - val_loss: 1.5214 - val_acc: 0.5838\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.88108\n",
      "Epoch 12/30\n",
      "120/120 [==============================] - 780s 7s/step - loss: 0.1288 - acc: 0.9516 - val_loss: 0.5121 - val_acc: 0.8432\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.88108\n",
      "Epoch 13/30\n",
      "120/120 [==============================] - 777s 6s/step - loss: 0.1419 - acc: 0.9454 - val_loss: 0.3516 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.88108\n",
      "Epoch 14/30\n",
      "120/120 [==============================] - 766s 6s/step - loss: 0.1328 - acc: 0.9502 - val_loss: 0.3742 - val_acc: 0.8865\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.88108 to 0.88649, saving model to D:\\Python\\cv\\keras_model\\my_model-14epoch-0.89acc.hdf5\n",
      "Epoch 15/30\n",
      "120/120 [==============================] - 759s 6s/step - loss: 0.1554 - acc: 0.9436 - val_loss: 0.5308 - val_acc: 0.8757\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.88649\n",
      "Epoch 16/30\n",
      "120/120 [==============================] - 726s 6s/step - loss: 0.1225 - acc: 0.9547 - val_loss: 0.2584 - val_acc: 0.8919\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.88649 to 0.89189, saving model to D:\\Python\\cv\\keras_model\\my_model-16epoch-0.89acc.hdf5\n",
      "Epoch 17/30\n",
      "120/120 [==============================] - 693s 6s/step - loss: 0.1322 - acc: 0.9509 - val_loss: 0.3975 - val_acc: 0.8646\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.89189\n",
      "Epoch 18/30\n",
      "120/120 [==============================] - 697s 6s/step - loss: 0.1249 - acc: 0.9549 - val_loss: 0.6566 - val_acc: 0.7838\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.89189\n",
      "Epoch 19/30\n",
      "120/120 [==============================] - 680s 6s/step - loss: 0.1346 - acc: 0.9474 - val_loss: 0.5248 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.89189\n",
      "Epoch 20/30\n",
      "120/120 [==============================] - 727s 6s/step - loss: 0.1324 - acc: 0.9486 - val_loss: 0.3561 - val_acc: 0.8649\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.89189\n",
      "Epoch 21/30\n",
      "120/120 [==============================] - 726s 6s/step - loss: 0.1200 - acc: 0.9560 - val_loss: 2.9221 - val_acc: 0.5104\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.89189\n",
      "Epoch 22/30\n",
      "120/120 [==============================] - 707s 6s/step - loss: 0.1261 - acc: 0.9515 - val_loss: 0.3647 - val_acc: 0.8811\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.89189\n",
      "Epoch 23/30\n",
      "120/120 [==============================] - 729s 6s/step - loss: 0.1186 - acc: 0.9568 - val_loss: 0.6106 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.89189\n",
      "Epoch 24/30\n",
      "120/120 [==============================] - 777s 6s/step - loss: 0.1222 - acc: 0.9553 - val_loss: 0.8826 - val_acc: 0.6649\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.89189\n",
      "Epoch 25/30\n",
      "120/120 [==============================] - 735s 6s/step - loss: 0.1197 - acc: 0.9544 - val_loss: 1.8751 - val_acc: 0.5365\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.89189\n",
      "Epoch 26/30\n",
      "120/120 [==============================] - 747s 6s/step - loss: 0.1163 - acc: 0.9564 - val_loss: 0.4514 - val_acc: 0.8649\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.89189\n",
      "Epoch 27/30\n",
      "120/120 [==============================] - 746s 6s/step - loss: 0.1198 - acc: 0.9565 - val_loss: 0.4236 - val_acc: 0.8324\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.89189\n",
      "Epoch 28/30\n",
      "120/120 [==============================] - 720s 6s/step - loss: 0.1126 - acc: 0.9579 - val_loss: 0.4258 - val_acc: 0.8486\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.89189\n",
      "Epoch 29/30\n",
      " 27/120 [=====>........................] - ETA: 10:22 - loss: 0.1306 - acc: 0.9537"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m                 \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalAveragePooling2D,Dropout,BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "size=224\n",
    "train=ImageDataGenerator(rescale=1./255,\n",
    "                                 horizontal_flip=True,\n",
    "                                   width_shift_range = 0.1,\n",
    "                                   height_shift_range = 0.1)\n",
    "val=ImageDataGenerator(rescale=1./255)\n",
    "train_generator=train.flow_from_directory(directory='d:\\Python\\cv\\\\model_data',\n",
    "                                                  target_size=(size,size),\n",
    "                                                  batch_size=64,\n",
    "                                                  class_mode='categorical')\n",
    "val_generator=val.flow_from_directory(directory='d:\\Python\\cv\\\\test_data',\n",
    "                                                target_size=(size,size),\n",
    "                                                batch_size=64,\n",
    "                                                class_mode='categorical')\n",
    "from keras.models import load_model\n",
    "model = load_model('D:\\Python\\cv\\keras_model\\\\my_model-18epoch-0.96acc.hdf5')\n",
    "\n",
    "path='D:\\Python\\\\cv\\\\keras_model\\\\my_model-{epoch:02d}epoch-{val_acc:.2f}acc.hdf5'\n",
    "checkpointer=ModelCheckpoint(path,monitor='val_acc',save_best_only=True,verbose=1)\n",
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=120,\n",
    "                   epochs=30,\n",
    "                   validation_data=val_generator,\n",
    "                   validation_steps=3,\n",
    "                   callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7705 images belonging to 2 classes.\n",
      "Found 121 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      " 19/100 [====>.........................] - ETA: 1:12:59 - loss: 0.8262 - acc: 0.6965"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from PIL import Image,ImageFile\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalAveragePooling2D,Dropout,BatchNormalization,Conv2D,MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "size=331\n",
    "train=ImageDataGenerator(rescale=1./255,\n",
    "                                 horizontal_flip=True,\n",
    "                                   width_shift_range = 0.1,\n",
    "                                   height_shift_range = 0.1)\n",
    "val=ImageDataGenerator(rescale=1./255)\n",
    "train_generator=train.flow_from_directory(directory='d:\\Python\\cv\\\\model_data',\n",
    "                                                  target_size=(size,size),\n",
    "                                                  batch_size=64,\n",
    "                                                  class_mode='categorical')\n",
    "val_generator=val.flow_from_directory(directory='d:\\Python\\cv\\\\test_data',\n",
    "                                                target_size=(size,size),\n",
    "                                                batch_size=64,\n",
    "                                                class_mode='categorical')\n",
    "\n",
    "input_shape=(size,size,3)\n",
    "cnn4 = Sequential()\n",
    "cnn4.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "cnn4.add(BatchNormalization())\n",
    "\n",
    "cnn4.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "cnn4.add(BatchNormalization())\n",
    "cnn4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn4.add(Dropout(0.25))\n",
    "\n",
    "cnn4.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "cnn4.add(BatchNormalization())\n",
    "cnn4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn4.add(Dropout(0.25))\n",
    "\n",
    "cnn4.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "cnn4.add(BatchNormalization())\n",
    "cnn4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn4.add(Dropout(0.25))\n",
    "\n",
    "cnn4.add(Flatten())\n",
    "\n",
    "cnn4.add(Dense(256, activation='relu'))\n",
    "cnn4.add(BatchNormalization())\n",
    "cnn4.add(Dropout(0.5))\n",
    "\n",
    "cnn4.add(Dense(128, activation='relu'))\n",
    "cnn4.add(BatchNormalization())\n",
    "cnn4.add(Dropout(0.5))\n",
    "\n",
    "cnn4.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "cnn4.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "path='D:\\Python\\\\cv\\\\keras_model\\\\3cnn_model-{epoch:02d}epoch-{val_acc:.2f}acc.hdf5'\n",
    "checkpointer=ModelCheckpoint(path,monitor='val_acc',save_best_only=True,verbose=1)\n",
    "\n",
    "cnn4.fit_generator(train_generator,\n",
    "                    steps_per_epoch=100,\n",
    "                   epochs=10,\n",
    "                   validation_data=val_generator,\n",
    "                   validation_steps=2,\n",
    "                   callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.5137255 , 0.50980395, 0.5803922 ],\n",
       "        [0.5137255 , 0.50980395, 0.5803922 ],\n",
       "        [0.5137255 , 0.50980395, 0.5803922 ],\n",
       "        ...,\n",
       "        [0.4392157 , 0.44705883, 0.49803922],\n",
       "        [0.43529412, 0.44313726, 0.49411765],\n",
       "        [0.42745098, 0.43529412, 0.4862745 ]],\n",
       "\n",
       "       [[0.52156866, 0.5176471 , 0.5882353 ],\n",
       "        [0.52156866, 0.5176471 , 0.5882353 ],\n",
       "        [0.52156866, 0.5176471 , 0.5882353 ],\n",
       "        ...,\n",
       "        [0.45882353, 0.46666667, 0.5176471 ],\n",
       "        [0.45490196, 0.4627451 , 0.5137255 ],\n",
       "        [0.4509804 , 0.45882353, 0.50980395]],\n",
       "\n",
       "       [[0.5294118 , 0.5254902 , 0.59607846],\n",
       "        [0.5294118 , 0.5254902 , 0.59607846],\n",
       "        [0.5294118 , 0.5254902 , 0.59607846],\n",
       "        ...,\n",
       "        [0.41568628, 0.42352942, 0.4745098 ],\n",
       "        [0.4117647 , 0.41960785, 0.47058824],\n",
       "        [0.40784314, 0.41568628, 0.46666667]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.02352941, 0.02352941, 0.02352941],\n",
       "        [0.02352941, 0.02352941, 0.02352941],\n",
       "        [0.02352941, 0.02352941, 0.02352941],\n",
       "        ...,\n",
       "        [0.14901961, 0.12156863, 0.05882353],\n",
       "        [0.14901961, 0.12156863, 0.05882353],\n",
       "        [0.14901961, 0.11764706, 0.06666667]],\n",
       "\n",
       "       [[0.02352941, 0.02352941, 0.02352941],\n",
       "        [0.02352941, 0.02352941, 0.02352941],\n",
       "        [0.02352941, 0.02352941, 0.02352941],\n",
       "        ...,\n",
       "        [0.14901961, 0.12156863, 0.05882353],\n",
       "        [0.14901961, 0.12156863, 0.05882353],\n",
       "        [0.14901961, 0.11764706, 0.06666667]],\n",
       "\n",
       "       [[0.02352941, 0.02352941, 0.02352941],\n",
       "        [0.02352941, 0.02352941, 0.02352941],\n",
       "        [0.02352941, 0.02352941, 0.02352941],\n",
       "        ...,\n",
       "        [0.14901961, 0.12156863, 0.05882353],\n",
       "        [0.14901961, 0.12156863, 0.05882353],\n",
       "        [0.14901961, 0.11764706, 0.06666667]]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='d:\\Python\\cv\\\\test_data\\pictures_good'\n",
    "im=Image.open(os.path.join(path,'2868163_2.jpg'))\n",
    "im=image.img_to_array(im)\n",
    "im/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2922 images belonging to 2 classes.\n",
      "Found 119 images belonging to 2 classes.\n",
      "Epoch 1/3\n",
      "100/100 [==============================] - 3370s 34s/step - loss: 0.2145 - acc: 0.9577 - val_loss: 0.4157 - val_acc: 0.8926\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 2983s 30s/step - loss: 0.1594 - acc: 0.9597 - val_loss: 0.3413 - val_acc: 0.8725\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 3021s 30s/step - loss: 0.1476 - acc: 0.9603 - val_loss: 0.4053 - val_acc: 0.8725\n",
      "Wall time: 2h 36min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from PIL import Image,ImageFile\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.applications.nasnet import NASNetLarge\n",
    "from keras.applications.nasnet import preprocess_input\n",
    "from keras.applications.nasnet import decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalAveragePooling2D,Conv2D,Dropout\n",
    "size=331\n",
    "data_generator=ImageDataGenerator(preprocessing_function=preprocess_input,rescale=1./255,\n",
    "                                 horizontal_flip=True,\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2)\n",
    "train_generator=data_generator.flow_from_directory(directory='d:\\Python\\cv\\\\model_data',\n",
    "                                                  target_size=(size,size),\n",
    "                                                  batch_size=30,\n",
    "                                                  class_mode='categorical')\n",
    "val_generator=data_generator.flow_from_directory(directory='d:\\Python\\cv\\\\test_data',\n",
    "                                                target_size=(size,size),\n",
    "                                                batch_size=30,\n",
    "                                                class_mode='categorical')\n",
    "model=Sequential()\n",
    "model.add(Conv2D(50,kernel_size=(3,3),input_shape=(size,size,3),activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(25,kernel_size=(3,3),activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(25,kernel_size=(3,3),activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(40,activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "#model.layers[0].trainable=False\n",
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=100,\n",
    "                   epochs=3,\n",
    "                   validation_data=val_generator,\n",
    "                   validation_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Dropout, Activation, Concatenate, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, GlobalAveragePooling2D, AveragePooling2D\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "\n",
    "def DenseNet(input_shape=None, dense_blocks=3, dense_layers=-1, growth_rate=12, nb_classes=None, dropout_rate=None,\n",
    "             bottleneck=False, compression=1.0, weight_decay=1e-4, depth=40):\n",
    "    \"\"\"\n",
    "    Creating a DenseNet\n",
    "    \n",
    "    Arguments:\n",
    "        input_shape  : shape of the input images. E.g. (28,28,1) for MNIST    \n",
    "        dense_blocks : amount of dense blocks that will be created (default: 3)    \n",
    "        dense_layers : number of layers in each dense block. You can also use a list for numbers of layers [2,4,3]\n",
    "                       or define only 2 to add 2 layers at all dense blocks. -1 means that dense_layers will be calculated\n",
    "                       by the given depth (default: -1)\n",
    "        growth_rate  : number of filters to add per dense block (default: 12)\n",
    "        nb_classes   : number of classes\n",
    "        dropout_rate : defines the dropout rate that is accomplished after each conv layer (except the first one).\n",
    "                       In the paper the authors recommend a dropout of 0.2 (default: None)\n",
    "        bottleneck   : (True / False) if true it will be added in convolution block (default: False)\n",
    "        compression  : reduce the number of feature-maps at transition layer. In the paper the authors recomment a compression\n",
    "                       of 0.5 (default: 1.0 - will have no compression effect)\n",
    "        weight_decay : weight decay of L2 regularization on weights (default: 1e-4)\n",
    "        depth        : number or layers (default: 40)\n",
    "        \n",
    "    Returns:\n",
    "        Model        : A Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    if nb_classes==None:\n",
    "        raise Exception('Please define number of classes (e.g. num_classes=10). This is required for final softmax.')\n",
    "    \n",
    "    if compression <=0.0 or compression > 1.0:\n",
    "        raise Exception('Compression have to be a value between 0.0 and 1.0. If you set compression to 1.0 it will be turn off.')\n",
    "    \n",
    "    if type(dense_layers) is list:\n",
    "        if len(dense_layers) != dense_blocks:\n",
    "            raise AssertionError('Number of dense blocks have to be same length to specified layers')\n",
    "    elif dense_layers == -1:\n",
    "        if bottleneck:\n",
    "            dense_layers = (depth - (dense_blocks + 1))/dense_blocks // 2\n",
    "        else:\n",
    "            dense_layers = (depth - (dense_blocks + 1))//dense_blocks\n",
    "        dense_layers = [int(dense_layers) for _ in range(dense_blocks)]\n",
    "    else:\n",
    "        dense_layers = [int(dense_layers) for _ in range(dense_blocks)]\n",
    "        \n",
    "    img_input = Input(shape=input_shape)\n",
    "    nb_channels = growth_rate * 2\n",
    "    \n",
    "    print('Creating DenseNet')\n",
    "    print('#############################################')\n",
    "    print('Dense blocks: %s' % dense_blocks)\n",
    "    print('Layers per dense block: %s' % dense_layers)\n",
    "    print('#############################################')\n",
    "    \n",
    "    # Initial convolution layer\n",
    "    x = Conv2D(nb_channels, (3,3), padding='same',strides=(1,1),\n",
    "                      use_bias=False, kernel_regularizer=l2(weight_decay))(img_input)\n",
    "    \n",
    "    # Building dense blocks\n",
    "    for block in range(dense_blocks):\n",
    "        \n",
    "        # Add dense block\n",
    "        x, nb_channels = dense_block(x, dense_layers[block], nb_channels, growth_rate, dropout_rate, bottleneck, weight_decay)\n",
    "        \n",
    "        if block < dense_blocks - 1:  # if it's not the last dense block\n",
    "            # Add transition_block\n",
    "            x = transition_layer(x, nb_channels, dropout_rate, compression, weight_decay)\n",
    "            nb_channels = int(nb_channels * compression)\n",
    "    \n",
    "    x = BatchNormalization(gamma_regularizer=l2(weight_decay), beta_regularizer=l2(weight_decay))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "#    x = Dense(nb_classes, activation='softmax', kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay))(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model_name = None\n",
    "    if growth_rate >= 36:\n",
    "        model_name = 'widedense'\n",
    "    else:\n",
    "        model_name = 'dense'\n",
    "        \n",
    "    if bottleneck:\n",
    "        model_name = model_name + 'b'\n",
    "        \n",
    "    if compression < 1.0:\n",
    "        model_name = model_name + 'c'\n",
    "        \n",
    "    return Model(img_input, x, name=model_name), model_name\n",
    "\n",
    "\n",
    "def dense_block(x, nb_layers, nb_channels, growth_rate, dropout_rate=None, bottleneck=False, weight_decay=1e-4):\n",
    "    \"\"\"\n",
    "    Creates a dense block and concatenates inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    x_list = [x]\n",
    "    for i in range(nb_layers):\n",
    "        cb = convolution_block(x, growth_rate, dropout_rate, bottleneck, weight_decay)\n",
    "        x_list.append(cb)\n",
    "        x = Concatenate(axis=-1)(x_list)\n",
    "        nb_channels += growth_rate\n",
    "    return x, nb_channels\n",
    "\n",
    "\n",
    "def convolution_block(x, nb_channels, dropout_rate=None, bottleneck=False, weight_decay=1e-4):\n",
    "    \"\"\"\n",
    "    Creates a convolution block consisting of BN-ReLU-Conv.\n",
    "    Optional: bottleneck, dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    # Bottleneck\n",
    "    if bottleneck:\n",
    "        bottleneckWidth = 4\n",
    "        x = BatchNormalization(gamma_regularizer=l2(weight_decay), beta_regularizer=l2(weight_decay))(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2D(nb_channels * bottleneckWidth, (1, 1), use_bias=False, kernel_regularizer=l2(weight_decay))(x)\n",
    "        # Dropout\n",
    "        if dropout_rate:\n",
    "            x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Standard (BN-ReLU-Conv)\n",
    "    x = BatchNormalization(gamma_regularizer=l2(weight_decay), beta_regularizer=l2(weight_decay))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(nb_channels, (3, 3), padding='same', use_bias=False, kernel_regularizer=l2(weight_decay))(x)\n",
    "    \n",
    "    # Dropout\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def transition_layer(x, nb_channels, dropout_rate=None, compression=1.0, weight_decay=1e-4):\n",
    "    \"\"\"\n",
    "    Creates a transition layer between dense blocks as transition, which do convolution and pooling.\n",
    "    Works as downsampling.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = BatchNormalization(gamma_regularizer=l2(weight_decay), beta_regularizer=l2(weight_decay))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(int(nb_channels*compression), (1, 1), padding='same',\n",
    "                      use_bias=False, kernel_regularizer=l2(weight_decay))(x)\n",
    "    \n",
    "    # Adding dropout\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DenseNet\n",
      "#############################################\n",
      "Dense blocks: 3\n",
      "Layers per dense block: [0, 0, 0]\n",
      "#############################################\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 4279 samples, validate on 121 samples\n",
      "Epoch 1/6\n",
      "4279/4279 [==============================] - 1985s 464ms/step - loss: 0.3757 - acc: 0.9390 - val_loss: 0.7291 - val_acc: 0.5868\n",
      "Epoch 2/6\n",
      "4279/4279 [==============================] - 1953s 456ms/step - loss: 0.2245 - acc: 0.9392 - val_loss: 2.1443 - val_acc: 0.8678\n",
      "Epoch 3/6\n",
      "4279/4279 [==============================] - 1923s 449ms/step - loss: 0.1991 - acc: 0.9406 - val_loss: 11.6341 - val_acc: 0.1322\n",
      "Epoch 4/6\n",
      "4279/4279 [==============================] - 2027s 474ms/step - loss: 0.1879 - acc: 0.9439 - val_loss: 13.8471 - val_acc: 0.1322\n",
      "Epoch 5/6\n",
      "4110/4279 [===========================>..] - ETA: 1:16 - loss: 0.1899 - acc: 0.9450"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-140730166634>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;31m#model.layers[0].trainable=False\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from PIL import Image,ImageFile\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.applications.nasnet import NASNetLarge\n",
    "from keras.applications.nasnet import preprocess_input\n",
    "from keras.applications.nasnet import decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalAveragePooling2D,Conv2D,Dropout,BatchNormalization\n",
    "\n",
    "size=331\n",
    "def parse_label(path):\n",
    "    data=pd.DataFrame(columns=['name','label'])\n",
    "    for folder in os.listdir(path):\n",
    "        for file in os.listdir(path+\"\\\\\"+folder):\n",
    "            data.loc[len(data)]=[file,folder]\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    return data\n",
    "train_data=parse_label('d:\\Python\\cv\\\\model_data')\n",
    "val_data=parse_label('d:\\Python\\cv\\\\test_data')\n",
    "def prepare_x(data,path):\n",
    "    X=np.zeros((len(data),size,size,3))\n",
    "    count=0\n",
    "    for fig,label in zip(data['name'].values,data['label'].values):\n",
    "        img=image.load_img(path+'\\\\'+label+'\\\\'+fig,target_size=(size,size,3))\n",
    "        img=image.img_to_array(img)/255\n",
    "        img=preprocess_input(img)\n",
    "        X[count]=img\n",
    "        count+=1\n",
    "    return X\n",
    "train_x=prepare_x(train_data,'d:\\Python\\cv\\\\model_data')\n",
    "val_x=prepare_x(val_data,'d:\\Python\\cv\\\\test_data')\n",
    "\n",
    "train_data['label']=train_data['label'].replace({'pictures_bad':1,'pictures_good':0})\n",
    "val_data['label']=val_data['label'].replace({'pictures_bad':1,'pictures_good':0})\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(train_data['label']),\n",
    "                                                 train_data['label'])\n",
    "\n",
    "model,model_name=DenseNet(input_shape=(size,size,3),nb_classes=2,dropout_rate=0.2,depth=7)\n",
    "#model.layers[0].trainable=False\n",
    "model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(train_x,train_data['label'].values,batch_size=30, epochs=6,class_weight=class_weights, validation_data=(val_x, val_data['label'].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 4279 samples, validate on 121 samples\n",
      "Epoch 1/6\n",
      "4279/4279 [==============================] - 2074s 485ms/step - loss: 0.2059 - acc: 0.9388 - val_loss: 0.3908 - val_acc: 0.8678\n",
      "Epoch 2/6\n",
      "4279/4279 [==============================] - 2045s 478ms/step - loss: 0.1133 - acc: 0.9645 - val_loss: 0.3887 - val_acc: 0.8678\n",
      "Epoch 3/6\n",
      "4279/4279 [==============================] - 2073s 484ms/step - loss: 0.0896 - acc: 0.9741 - val_loss: 0.4002 - val_acc: 0.8678\n",
      "Epoch 4/6\n",
      "4279/4279 [==============================] - 2004s 468ms/step - loss: 0.0641 - acc: 0.9818 - val_loss: 0.4010 - val_acc: 0.8678\n",
      "Epoch 5/6\n",
      "4279/4279 [==============================] - 2040s 477ms/step - loss: 0.0628 - acc: 0.9804 - val_loss: 0.4054 - val_acc: 0.8678\n",
      "Epoch 6/6\n",
      "4279/4279 [==============================] - 2089s 488ms/step - loss: 0.0542 - acc: 0.9846 - val_loss: 0.4002 - val_acc: 0.8678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x649c7b00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image,ImageFile\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.applications.densenet import DenseNet169\n",
    "from keras.applications.densenet import preprocess_input\n",
    "from keras.applications.densenet import decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalAveragePooling2D,Conv2D,Dropout,BatchNormalization\n",
    "size=224\n",
    "def parse_label(path):\n",
    "    data=pd.DataFrame(columns=['name','label'])\n",
    "    for folder in os.listdir(path):\n",
    "        for file in os.listdir(path+\"\\\\\"+folder):\n",
    "            data.loc[len(data)]=[file,folder]\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    return data\n",
    "train_data=parse_label('d:\\Python\\cv\\\\model_data')\n",
    "val_data=parse_label('d:\\Python\\cv\\\\test_data')\n",
    "def prepare_x(data,path):\n",
    "    X=np.zeros((len(data),size,size,3))\n",
    "    count=0\n",
    "    for fig,label in zip(data['name'].values,data['label'].values):\n",
    "        img=image.load_img(path+'\\\\'+label+'\\\\'+fig,target_size=(size,size,3))\n",
    "        img=image.img_to_array(img)/255\n",
    "        img=preprocess_input(img)\n",
    "        X[count]=img\n",
    "        count+=1\n",
    "    return X\n",
    "train_x=prepare_x(train_data,'d:\\Python\\cv\\\\model_data')\n",
    "val_x=prepare_x(val_data,'d:\\Python\\cv\\\\test_data')\n",
    "\n",
    "train_data['label']=train_data['label'].replace({'pictures_bad':1,'pictures_good':0})\n",
    "val_data['label']=val_data['label'].replace({'pictures_bad':1,'pictures_good':0})\n",
    "\n",
    "model=Sequential()\n",
    "model.add(DenseNet169(input_shape=(size,size,3),include_top=False,pooling='max'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(12,activation='relu'))        \n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.layers[0].trainable=False\n",
    "model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(train_x,train_data['label'].values,batch_size=40, epochs=6, validation_data=(val_x, val_data['label'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3055 samples, validate on 119 samples\n",
      "Epoch 1/30\n",
      "3055/3055 [==============================] - 7s 2ms/step - loss: 8.3715 - acc: 0.4730 - val_loss: 14.0668 - val_acc: 0.1176\n",
      "Epoch 2/30\n",
      "3055/3055 [==============================] - 0s 107us/step - loss: 6.5322 - acc: 0.5885 - val_loss: 10.0669 - val_acc: 0.3697\n",
      "Epoch 3/30\n",
      "3055/3055 [==============================] - 0s 118us/step - loss: 4.1682 - acc: 0.7365 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 4/30\n",
      "3055/3055 [==============================] - 0s 119us/step - loss: 2.4943 - acc: 0.8426 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 5/30\n",
      "3055/3055 [==============================] - 1s 165us/step - loss: 2.2247 - acc: 0.8599 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 6/30\n",
      "3055/3055 [==============================] - 0s 106us/step - loss: 1.7964 - acc: 0.8874 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 7/30\n",
      "3055/3055 [==============================] - 0s 114us/step - loss: 1.7290 - acc: 0.8913 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 8/30\n",
      "3055/3055 [==============================] - 0s 115us/step - loss: 1.6143 - acc: 0.8982 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 9/30\n",
      "3055/3055 [==============================] - 0s 106us/step - loss: 1.7399 - acc: 0.8907 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 10/30\n",
      "3055/3055 [==============================] - 0s 109us/step - loss: 1.7001 - acc: 0.8926 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 11/30\n",
      "3055/3055 [==============================] - 0s 109us/step - loss: 1.7909 - acc: 0.8871 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 12/30\n",
      "3055/3055 [==============================] - 0s 123us/step - loss: 1.6829 - acc: 0.8943 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 13/30\n",
      "3055/3055 [==============================] - 0s 145us/step - loss: 1.7328 - acc: 0.8907 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 14/30\n",
      "3055/3055 [==============================] - 0s 120us/step - loss: 1.6946 - acc: 0.8933 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 15/30\n",
      "3055/3055 [==============================] - 0s 110us/step - loss: 1.6770 - acc: 0.8949 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 16/30\n",
      "3055/3055 [==============================] - 0s 114us/step - loss: 1.5903 - acc: 0.8998 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 17/30\n",
      "3055/3055 [==============================] - 0s 118us/step - loss: 1.6052 - acc: 0.8992 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 18/30\n",
      "3055/3055 [==============================] - 0s 109us/step - loss: 1.6454 - acc: 0.8969 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 19/30\n",
      "3055/3055 [==============================] - 0s 157us/step - loss: 1.5354 - acc: 0.9034 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 20/30\n",
      "3055/3055 [==============================] - 0s 124us/step - loss: 1.5994 - acc: 0.8998 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 21/30\n",
      "3055/3055 [==============================] - 0s 116us/step - loss: 1.6481 - acc: 0.8966 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 22/30\n",
      "3055/3055 [==============================] - 0s 110us/step - loss: 1.5237 - acc: 0.9044 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 23/30\n",
      "3055/3055 [==============================] - 0s 117us/step - loss: 1.6824 - acc: 0.8943 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 24/30\n",
      "3055/3055 [==============================] - 0s 128us/step - loss: 1.6533 - acc: 0.8956 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 25/30\n",
      "3055/3055 [==============================] - 1s 210us/step - loss: 1.4667 - acc: 0.9080 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 26/30\n",
      "3055/3055 [==============================] - 0s 142us/step - loss: 1.5177 - acc: 0.9047 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 27/30\n",
      "3055/3055 [==============================] - 0s 156us/step - loss: 1.4751 - acc: 0.9074 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 28/30\n",
      "3055/3055 [==============================] - 0s 102us/step - loss: 1.5370 - acc: 0.9034 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 29/30\n",
      "3055/3055 [==============================] - 0s 98us/step - loss: 1.4722 - acc: 0.9077 - val_loss: 1.8962 - val_acc: 0.8824\n",
      "Epoch 30/30\n",
      "3055/3055 [==============================] - 0s 92us/step - loss: 1.4162 - acc: 0.9110 - val_loss: 1.8962 - val_acc: 0.8824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1028787b8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "path='d:\\Python\\cv\\\\model_data\\pictures_bad'\n",
    "im=Image.open(os.path.join(path,'pexels-photo-289237.jpeg'))\n",
    "im=im.resize((size, size), Image.ANTIALIAS)\n",
    "im=image.img_to_array(im)\n",
    "hist = cv2.calcHist([im],[0],None,[256],[0,256])\n",
    "#np.squeeze(hist)\n",
    "\n",
    "from PIL import Image,ImageFile\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.applications.nasnet import NASNetLarge\n",
    "from keras.applications.nasnet import preprocess_input\n",
    "from keras.applications.nasnet import decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Flatten, GlobalAveragePooling2D,Conv2D,Dropout,BatchNormalization,Input\n",
    "size=331\n",
    "def parse_label(path):\n",
    "    data=pd.DataFrame(columns=['name','label'])\n",
    "    for folder in os.listdir(path):\n",
    "        for file in os.listdir(path+\"\\\\\"+folder):\n",
    "            data.loc[len(data)]=[file,folder]\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    return data\n",
    "train_data=parse_label('d:\\Python\\cv\\\\model_data')\n",
    "test_data=parse_label('d:\\Python\\cv\\\\test_data')\n",
    "def prepare_x(data,path):\n",
    "    X=np.zeros((len(data),256))\n",
    "    count=0\n",
    "    for fig,label in zip(data['name'].values,data['label'].values):\n",
    "        img=image.load_img(path+'\\\\'+label+'\\\\'+fig,target_size=(331,331,3))\n",
    "        img=image.img_to_array(img)\n",
    "        img=preprocess_input(img)\n",
    "        X[count]=np.squeeze(cv2.calcHist([img],[0],None,[256],[0,256]))\n",
    "        count+=1\n",
    "    return X\n",
    "train_x=prepare_x(train_data,'d:\\Python\\cv\\\\model_data')\n",
    "test_x=prepare_x(test_data,'d:\\Python\\cv\\\\test_data')\n",
    "\n",
    "train_data['label']=train_data['label'].replace({'pictures_bad':1,'pictures_good':0})\n",
    "test_data['label']=test_data['label'].replace({'pictures_bad':1,'pictures_good':0})\n",
    "\n",
    "inp=Input(shape=(256,))\n",
    "x=Dense(128,activation='relu')(inp)\n",
    "x=Dropout(0.35)(x)\n",
    "x=Dense(64,activation='relu')(x)\n",
    "x=Dropout(0.35)(x)\n",
    "x=Dense(32,activation='relu')(x)\n",
    "x=Dropout(0.35)(x)\n",
    "x=Dense(16,activation='relu')(x)\n",
    "x=Dropout(0.35)(x)\n",
    "out=Dense(1,activation='sigmoid')(x)\n",
    "model=Model(inputs=inp,outputs=out)\n",
    "model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(train_x,train_data['label'].values,batch_size=40, epochs=30, validation_data=(val_x, val_data['label'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9484452\ttest: 0.9427169\tbest: 0.9427169 (0)\ttotal: 64.1ms\tremaining: 1m 4s\n",
      "100:\tlearn: 0.9680851\ttest: 0.9639935\tbest: 0.9639935 (100)\ttotal: 1.24s\tremaining: 11.1s\n",
      "200:\tlearn: 0.9701309\ttest: 0.9639935\tbest: 0.9639935 (100)\ttotal: 2.29s\tremaining: 9.13s\n",
      "300:\tlearn: 0.9734043\ttest: 0.9639935\tbest: 0.9639935 (100)\ttotal: 3.37s\tremaining: 7.82s\n",
      "400:\tlearn: 0.9754501\ttest: 0.9639935\tbest: 0.9639935 (100)\ttotal: 4.42s\tremaining: 6.61s\n",
      "500:\tlearn: 0.9758592\ttest: 0.9623568\tbest: 0.9639935 (100)\ttotal: 5.51s\tremaining: 5.49s\n",
      "600:\tlearn: 0.9766776\ttest: 0.9623568\tbest: 0.9639935 (100)\ttotal: 6.61s\tremaining: 4.38s\n",
      "700:\tlearn: 0.9779051\ttest: 0.9607201\tbest: 0.9639935 (100)\ttotal: 7.68s\tremaining: 3.27s\n",
      "800:\tlearn: 0.9779051\ttest: 0.9607201\tbest: 0.9639935 (100)\ttotal: 8.71s\tremaining: 2.16s\n",
      "900:\tlearn: 0.9787234\ttest: 0.9607201\tbest: 0.9639935 (100)\ttotal: 9.74s\tremaining: 1.07s\n",
      "999:\tlearn: 0.9787234\ttest: 0.9607201\tbest: 0.9639935 (100)\ttotal: 10.8s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9639934534\n",
      "bestIteration = 100\n",
      "\n",
      "Shrink model to first 101 iterations.\n",
      "0:\tlearn: 0.9496727\ttest: 0.9443535\tbest: 0.9443535 (0)\ttotal: 13.1ms\tremaining: 13.1s\n",
      "100:\tlearn: 0.9664484\ttest: 0.9623568\tbest: 0.9623568 (100)\ttotal: 1.18s\tremaining: 10.5s\n",
      "200:\tlearn: 0.9684943\ttest: 0.9623568\tbest: 0.9623568 (100)\ttotal: 2.34s\tremaining: 9.29s\n",
      "300:\tlearn: 0.9705401\ttest: 0.9590835\tbest: 0.9623568 (100)\ttotal: 3.32s\tremaining: 7.71s\n",
      "400:\tlearn: 0.9713584\ttest: 0.9590835\tbest: 0.9623568 (100)\ttotal: 4.32s\tremaining: 6.45s\n",
      "500:\tlearn: 0.9717676\ttest: 0.9574468\tbest: 0.9623568 (100)\ttotal: 5.36s\tremaining: 5.34s\n",
      "600:\tlearn: 0.9725859\ttest: 0.9574468\tbest: 0.9623568 (100)\ttotal: 6.63s\tremaining: 4.4s\n",
      "700:\tlearn: 0.9738134\ttest: 0.9574468\tbest: 0.9623568 (100)\ttotal: 7.69s\tremaining: 3.28s\n",
      "800:\tlearn: 0.9742226\ttest: 0.9574468\tbest: 0.9623568 (100)\ttotal: 8.87s\tremaining: 2.2s\n",
      "900:\tlearn: 0.9754501\ttest: 0.9574468\tbest: 0.9623568 (100)\ttotal: 10.3s\tremaining: 1.13s\n",
      "999:\tlearn: 0.9754501\ttest: 0.9574468\tbest: 0.9623568 (100)\ttotal: 12.1s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9623567921\n",
      "bestIteration = 100\n",
      "\n",
      "Shrink model to first 101 iterations.\n",
      "0:\tlearn: 0.9472177\ttest: 0.9541735\tbest: 0.9541735 (0)\ttotal: 38.1ms\tremaining: 38.1s\n",
      "100:\tlearn: 0.9664484\ttest: 0.9656301\tbest: 0.9656301 (100)\ttotal: 1.75s\tremaining: 15.6s\n",
      "200:\tlearn: 0.9701309\ttest: 0.9639935\tbest: 0.9656301 (100)\ttotal: 3.17s\tremaining: 12.6s\n",
      "300:\tlearn: 0.9729951\ttest: 0.9590835\tbest: 0.9656301 (100)\ttotal: 4.34s\tremaining: 10.1s\n",
      "400:\tlearn: 0.9734043\ttest: 0.9590835\tbest: 0.9656301 (100)\ttotal: 5.56s\tremaining: 8.3s\n",
      "500:\tlearn: 0.9738134\ttest: 0.9590835\tbest: 0.9656301 (100)\ttotal: 6.74s\tremaining: 6.71s\n",
      "600:\tlearn: 0.9738134\ttest: 0.9590835\tbest: 0.9656301 (100)\ttotal: 7.99s\tremaining: 5.31s\n",
      "700:\tlearn: 0.9766776\ttest: 0.9574468\tbest: 0.9656301 (100)\ttotal: 9.18s\tremaining: 3.92s\n",
      "800:\tlearn: 0.9779051\ttest: 0.9574468\tbest: 0.9656301 (100)\ttotal: 10.4s\tremaining: 2.57s\n",
      "900:\tlearn: 0.9791326\ttest: 0.9558101\tbest: 0.9656301 (100)\ttotal: 11.6s\tremaining: 1.28s\n",
      "999:\tlearn: 0.9795417\ttest: 0.9574468\tbest: 0.9656301 (100)\ttotal: 12.9s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9656301146\n",
      "bestIteration = 100\n",
      "\n",
      "Shrink model to first 101 iterations.\n",
      "0:\tlearn: 0.9484452\ttest: 0.9476268\tbest: 0.9476268 (0)\ttotal: 12.9ms\tremaining: 12.9s\n",
      "100:\tlearn: 0.9668576\ttest: 0.9639935\tbest: 0.9639935 (100)\ttotal: 1.28s\tremaining: 11.4s\n",
      "200:\tlearn: 0.9684943\ttest: 0.9656301\tbest: 0.9656301 (200)\ttotal: 2.5s\tremaining: 9.92s\n",
      "300:\tlearn: 0.9721768\ttest: 0.9656301\tbest: 0.9656301 (200)\ttotal: 3.75s\tremaining: 8.7s\n",
      "400:\tlearn: 0.9742226\ttest: 0.9639935\tbest: 0.9656301 (200)\ttotal: 4.96s\tremaining: 7.42s\n",
      "500:\tlearn: 0.9750409\ttest: 0.9656301\tbest: 0.9656301 (200)\ttotal: 6.13s\tremaining: 6.1s\n",
      "600:\tlearn: 0.9762684\ttest: 0.9639935\tbest: 0.9656301 (200)\ttotal: 7.35s\tremaining: 4.88s\n",
      "700:\tlearn: 0.9758592\ttest: 0.9639935\tbest: 0.9656301 (200)\ttotal: 8.52s\tremaining: 3.63s\n",
      "800:\tlearn: 0.9774959\ttest: 0.9639935\tbest: 0.9656301 (200)\ttotal: 9.68s\tremaining: 2.4s\n",
      "900:\tlearn: 0.9787234\ttest: 0.9639935\tbest: 0.9656301 (200)\ttotal: 10.8s\tremaining: 1.19s\n",
      "999:\tlearn: 0.9791326\ttest: 0.9639935\tbest: 0.9656301 (200)\ttotal: 12.1s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9656301146\n",
      "bestIteration = 200\n",
      "\n",
      "Shrink model to first 201 iterations.\n",
      "0:\tlearn: 0.9484452\ttest: 0.9509002\tbest: 0.9509002 (0)\ttotal: 15.3ms\tremaining: 15.3s\n",
      "100:\tlearn: 0.9676759\ttest: 0.9623568\tbest: 0.9623568 (100)\ttotal: 1.3s\tremaining: 11.6s\n",
      "200:\tlearn: 0.9697218\ttest: 0.9623568\tbest: 0.9623568 (100)\ttotal: 2.49s\tremaining: 9.91s\n",
      "300:\tlearn: 0.9725859\ttest: 0.9574468\tbest: 0.9623568 (100)\ttotal: 3.69s\tremaining: 8.57s\n",
      "400:\tlearn: 0.9742226\ttest: 0.9574468\tbest: 0.9623568 (100)\ttotal: 4.9s\tremaining: 7.32s\n",
      "500:\tlearn: 0.9750409\ttest: 0.9574468\tbest: 0.9623568 (100)\ttotal: 6.11s\tremaining: 6.08s\n",
      "600:\tlearn: 0.9754501\ttest: 0.9558101\tbest: 0.9623568 (100)\ttotal: 7.32s\tremaining: 4.86s\n",
      "700:\tlearn: 0.9774959\ttest: 0.9541735\tbest: 0.9623568 (100)\ttotal: 8.55s\tremaining: 3.65s\n",
      "800:\tlearn: 0.9783142\ttest: 0.9541735\tbest: 0.9623568 (100)\ttotal: 9.85s\tremaining: 2.45s\n",
      "900:\tlearn: 0.9791326\ttest: 0.9541735\tbest: 0.9623568 (100)\ttotal: 11.1s\tremaining: 1.21s\n",
      "999:\tlearn: 0.9803601\ttest: 0.9541735\tbest: 0.9623568 (100)\ttotal: 12.2s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9623567921\n",
      "bestIteration = 100\n",
      "\n",
      "Shrink model to first 101 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9639934533551555"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#catboost\n",
    "from PIL import Image,ImageFile\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.applications.nasnet import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "def parse_label(path):\n",
    "    data=pd.DataFrame(columns=['name','label'])\n",
    "    for folder in os.listdir(path):\n",
    "        for file in os.listdir(path+\"\\\\\"+folder):\n",
    "            data.loc[len(data)]=[file,folder]\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    return data\n",
    "train_data=parse_label( 'd:\\Python\\cv\\\\model_data')\n",
    "test_data=parse_label('d:\\Python\\cv\\\\test_data')\n",
    "def prepare_x(data,path):\n",
    "    X=np.zeros((len(data),256))\n",
    "    count=0\n",
    "    for fig,label in zip(data['name'].values,data['label'].values):\n",
    "        img=image.load_img(path+'\\\\'+label+'\\\\'+fig,target_size=(331,331,3))\n",
    "        img=image.img_to_array(img)\n",
    "        img=preprocess_input(img)\n",
    "        X[count]=np.squeeze(cv2.calcHist([img],[0],None,[256],[0,256]))\n",
    "        count+=1\n",
    "    return X\n",
    "\n",
    "train_x=prepare_x(train_data,'d:\\Python\\cv\\\\model_data')\n",
    "test_x=prepare_x(test_data,'d:\\Python\\cv\\\\test_data')\n",
    "train_data['label']=train_data['label'].replace({'pictures_bad':1,'pictures_good':0})\n",
    "test_data['label']=test_data['label'].replace({'pictures_bad':1,'pictures_good':0})\n",
    "\n",
    "from catboost import Pool,CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "num_folds=5\n",
    "rskf=StratifiedKFold(num_folds,shuffle=True)\n",
    "val_pr=np.zeros(len(train_x))\n",
    "test_pr=np.zeros(len(test_x))\n",
    "X=train_x.copy()\n",
    "y=train_data['label']\n",
    "for train_index,val_index in rskf.split(X,y):\n",
    "    train_x=X[train_index]\n",
    "    train_y=y[train_index]\n",
    "    val_x=X[val_index]\n",
    "    val_y=y[val_index]\n",
    "    pool=Pool(train_x,train_y)\n",
    "    val_pool=Pool(val_x,val_y)\n",
    "    model = CatBoostClassifier(iterations=1000, metric_period=100,loss_function='CrossEntropy', eval_metric='Accuracy')\n",
    "    model.fit(pool,eval_set=val_pool,use_best_model=True)\n",
    "    val_pr[val_index]=model.predict(val_x)\n",
    "    test_pr+=model.predict(test_x,prediction_type='Probability')[:,1]/num_folds\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, val_pr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53234635, 8.22884615])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_error: 0.0384615\tvalid_1's binary_error: 0.0441899\n",
      "[200]\ttraining's binary_error: 0.0315057\tvalid_1's binary_error: 0.0376432\n",
      "[300]\ttraining's binary_error: 0.0286416\tvalid_1's binary_error: 0.0360065\n",
      "[400]\ttraining's binary_error: 0.0257774\tvalid_1's binary_error: 0.0376432\n",
      "[500]\ttraining's binary_error: 0.0233224\tvalid_1's binary_error: 0.0392799\n",
      "[600]\ttraining's binary_error: 0.0212766\tvalid_1's binary_error: 0.0425532\n",
      "Early stopping, best iteration is:\n",
      "[264]\ttraining's binary_error: 0.0302782\tvalid_1's binary_error: 0.0360065\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_error: 0.0364157\tvalid_1's binary_error: 0.0458265\n",
      "[200]\ttraining's binary_error: 0.0315057\tvalid_1's binary_error: 0.0458265\n",
      "[300]\ttraining's binary_error: 0.0294599\tvalid_1's binary_error: 0.0458265\n",
      "[400]\ttraining's binary_error: 0.0249591\tvalid_1's binary_error: 0.0409165\n",
      "[500]\ttraining's binary_error: 0.0220949\tvalid_1's binary_error: 0.0409165\n",
      "[600]\ttraining's binary_error: 0.0192308\tvalid_1's binary_error: 0.0392799\n",
      "[700]\ttraining's binary_error: 0.0180033\tvalid_1's binary_error: 0.0392799\n",
      "[800]\ttraining's binary_error: 0.0159574\tvalid_1's binary_error: 0.0392799\n",
      "[900]\ttraining's binary_error: 0.0151391\tvalid_1's binary_error: 0.0376432\n",
      "[1000]\ttraining's binary_error: 0.01473\tvalid_1's binary_error: 0.0392799\n",
      "[1100]\ttraining's binary_error: 0.0143208\tvalid_1's binary_error: 0.0392799\n",
      "[1200]\ttraining's binary_error: 0.0139116\tvalid_1's binary_error: 0.0392799\n",
      "Early stopping, best iteration is:\n",
      "[896]\ttraining's binary_error: 0.0155483\tvalid_1's binary_error: 0.0376432\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_error: 0.0355974\tvalid_1's binary_error: 0.0360065\n",
      "[200]\ttraining's binary_error: 0.0343699\tvalid_1's binary_error: 0.0327332\n",
      "[300]\ttraining's binary_error: 0.0282324\tvalid_1's binary_error: 0.0376432\n",
      "[400]\ttraining's binary_error: 0.0257774\tvalid_1's binary_error: 0.0392799\n",
      "[500]\ttraining's binary_error: 0.0241408\tvalid_1's binary_error: 0.0409165\n",
      "Early stopping, best iteration is:\n",
      "[159]\ttraining's binary_error: 0.0327332\tvalid_1's binary_error: 0.0327332\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_error: 0.0400982\tvalid_1's binary_error: 0.0327332\n",
      "[200]\ttraining's binary_error: 0.0343699\tvalid_1's binary_error: 0.0245499\n",
      "[300]\ttraining's binary_error: 0.0294599\tvalid_1's binary_error: 0.0278232\n",
      "[400]\ttraining's binary_error: 0.0274141\tvalid_1's binary_error: 0.0261866\n",
      "[500]\ttraining's binary_error: 0.0265957\tvalid_1's binary_error: 0.0261866\n",
      "Early stopping, best iteration is:\n",
      "[150]\ttraining's binary_error: 0.0364157\tvalid_1's binary_error: 0.0229133\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_error: 0.0347791\tvalid_1's binary_error: 0.0474632\n",
      "[200]\ttraining's binary_error: 0.0298691\tvalid_1's binary_error: 0.0441899\n",
      "[300]\ttraining's binary_error: 0.0249591\tvalid_1's binary_error: 0.0425532\n",
      "[400]\ttraining's binary_error: 0.0229133\tvalid_1's binary_error: 0.0409165\n",
      "[500]\ttraining's binary_error: 0.0216858\tvalid_1's binary_error: 0.0409165\n",
      "[600]\ttraining's binary_error: 0.0200491\tvalid_1's binary_error: 0.0392799\n",
      "[700]\ttraining's binary_error: 0.0171849\tvalid_1's binary_error: 0.0392799\n",
      "[800]\ttraining's binary_error: 0.0151391\tvalid_1's binary_error: 0.0409165\n",
      "[900]\ttraining's binary_error: 0.0130933\tvalid_1's binary_error: 0.0441899\n",
      "Early stopping, best iteration is:\n",
      "[532]\ttraining's binary_error: 0.0212766\tvalid_1's binary_error: 0.0392799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9662847790507365"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image,ImageFile\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.applications.nasnet import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "def parse_label(path):\n",
    "    data=pd.DataFrame(columns=['name','label'])\n",
    "    for folder in os.listdir(path):\n",
    "        for file in os.listdir(path+\"\\\\\"+folder):\n",
    "            data.loc[len(data)]=[file,folder]\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    return data\n",
    "train_data=parse_label('d:\\Python\\cv\\\\model_data')\n",
    "test_data=parse_label('d:\\Python\\cv\\\\test_data')\n",
    "def prepare_x(data,path):\n",
    "    X=np.zeros((len(data),256))\n",
    "    count=0\n",
    "    for fig,label in zip(data['name'].values,data['label'].values):\n",
    "        img=image.load_img(path+'\\\\'+label+'\\\\'+fig,target_size=(331,331,3))\n",
    "        img=image.img_to_array(img)\n",
    "        img=preprocess_input(img)\n",
    "        X[count]=np.squeeze(cv2.calcHist([img],[0],None,[256],[0,256]))\n",
    "        count+=1\n",
    "    return X\n",
    "\n",
    "train_x=prepare_x(train_data,'d:\\Python\\cv\\\\model_data')\n",
    "test_x=prepare_x(test_data,'d:\\Python\\cv\\\\test_data')\n",
    "train_data['label']=train_data['label'].replace({'pictures_bad':1,'pictures_good':0})\n",
    "test_data['label']=test_data['label'].replace({'pictures_bad':1,'pictures_good':0})\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "num_folds=5\n",
    "params={'n_estimators': 6100, \n",
    "             'n_jobs': 6,\n",
    "         'objective':'binary',\n",
    "         'max_depth': 15,\n",
    "         'learning_rate': 0.01,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"metric\": 'binary_error',\n",
    "         \"random_state\": 2333\n",
    "    }\n",
    "rskf=StratifiedKFold(num_folds,shuffle=True)\n",
    "val_pr=np.zeros(len(train_x))\n",
    "test_pr=np.zeros(len(test_x))\n",
    "X=train_x.copy()\n",
    "y=train_data['label']\n",
    "for train_index,val_index in rskf.split(X,y):\n",
    "    train_x=X[train_index]\n",
    "    train_y=y[train_index]\n",
    "    val_x=X[val_index]\n",
    "    val_y=y[val_index]\n",
    "    pool=lgb.Dataset(train_x,label=train_y)\n",
    "    val_pool=lgb.Dataset(val_x,label=val_y)\n",
    "    num_round=10000\n",
    "    model = lgb.train(params,pool,num_round,valid_sets=[pool,val_pool],verbose_eval=100,early_stopping_rounds=400)\n",
    "    val_pr[val_index]=model.predict(val_x,num_iteration=model.best_iteration)\n",
    "    test_pr+=model.predict(test_x,num_iteration=model.best_iteration)/num_folds\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, np.where(val_pr>0.5,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_error: 0.0134385\tvalid_1's binary_error: 0.0151869\n",
      "[200]\ttraining's binary_error: 0.00584283\tvalid_1's binary_error: 0.0140187\n",
      "[300]\ttraining's binary_error: 0.000876424\tvalid_1's binary_error: 0.0140187\n",
      "[400]\ttraining's binary_error: 0.000584283\tvalid_1's binary_error: 0.0140187\n",
      "[500]\ttraining's binary_error: 0.000584283\tvalid_1's binary_error: 0.0140187\n",
      "Early stopping, best iteration is:\n",
      "[107]\ttraining's binary_error: 0.0134385\tvalid_1's binary_error: 0.0140187\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_error: 0.0148992\tvalid_1's binary_error: 0.0163551\n",
      "[200]\ttraining's binary_error: 0.00408998\tvalid_1's binary_error: 0.0163551\n",
      "[300]\ttraining's binary_error: 0.000584283\tvalid_1's binary_error: 0.0128505\n",
      "[400]\ttraining's binary_error: 0.000292141\tvalid_1's binary_error: 0.0175234\n",
      "[500]\ttraining's binary_error: 0.000292141\tvalid_1's binary_error: 0.0186916\n",
      "[600]\ttraining's binary_error: 0.000292141\tvalid_1's binary_error: 0.0186916\n",
      "Early stopping, best iteration is:\n",
      "[254]\ttraining's binary_error: 0.00146071\tvalid_1's binary_error: 0.0128505\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_error: 0.0131464\tvalid_1's binary_error: 0.0151869\n",
      "[200]\ttraining's binary_error: 0.00613497\tvalid_1's binary_error: 0.0151869\n",
      "[300]\ttraining's binary_error: 0.00116857\tvalid_1's binary_error: 0.0151869\n",
      "[400]\ttraining's binary_error: 0.000584283\tvalid_1's binary_error: 0.0163551\n",
      "[500]\ttraining's binary_error: 0.000584283\tvalid_1's binary_error: 0.0140187\n",
      "[600]\ttraining's binary_error: 0.000584283\tvalid_1's binary_error: 0.0151869\n",
      "[700]\ttraining's binary_error: 0.000584283\tvalid_1's binary_error: 0.0151869\n",
      "Early stopping, best iteration is:\n",
      "[338]\ttraining's binary_error: 0.000584283\tvalid_1's binary_error: 0.0151869\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_error: 0.0148992\tvalid_1's binary_error: 0.0280374\n",
      "[200]\ttraining's binary_error: 0.00701139\tvalid_1's binary_error: 0.0198598\n",
      "[300]\ttraining's binary_error: 0.000876424\tvalid_1's binary_error: 0.0163551\n",
      "[400]\ttraining's binary_error: 0.000292141\tvalid_1's binary_error: 0.0163551\n",
      "[500]\ttraining's binary_error: 0.000292141\tvalid_1's binary_error: 0.0163551\n",
      "[600]\ttraining's binary_error: 0.000292141\tvalid_1's binary_error: 0.0163551\n",
      "[700]\ttraining's binary_error: 0.000292141\tvalid_1's binary_error: 0.0151869\n",
      "Early stopping, best iteration is:\n",
      "[311]\ttraining's binary_error: 0.000292141\tvalid_1's binary_error: 0.0163551\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_error: 0.0146028\tvalid_1's binary_error: 0.0140351\n",
      "[200]\ttraining's binary_error: 0.00584112\tvalid_1's binary_error: 0.0128655\n",
      "[300]\ttraining's binary_error: 0.000876168\tvalid_1's binary_error: 0.0128655\n",
      "[400]\ttraining's binary_error: 0.000584112\tvalid_1's binary_error: 0.0128655\n",
      "[500]\ttraining's binary_error: 0.000584112\tvalid_1's binary_error: 0.0128655\n",
      "[600]\ttraining's binary_error: 0.000584112\tvalid_1's binary_error: 0.0128655\n",
      "[700]\ttraining's binary_error: 0.000584112\tvalid_1's binary_error: 0.0128655\n",
      "Early stopping, best iteration is:\n",
      "[308]\ttraining's binary_error: 0.000584112\tvalid_1's binary_error: 0.0128655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9857443327880346"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from catboost import Pool,CatBoostClassifier\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# num_folds=5\n",
    "# rskf=StratifiedKFold(num_folds,shuffle=True)\n",
    "# val_pr=np.zeros(len(X))\n",
    "# test_pr=np.zeros(len(test_x))\n",
    "# for train_index,val_index in rskf.split(X,y):\n",
    "#     train_x=X[train_index]\n",
    "#     train_y=y[train_index]\n",
    "#     val_x=X[val_index]\n",
    "#     val_y=y[val_index]\n",
    "#     pool=Pool(train_x,train_y)\n",
    "#     val_pool=Pool(val_x,val_y)\n",
    "#     model = CatBoostClassifier(iterations=1000, metric_period=100,loss_function='CrossEntropy', eval_metric='Accuracy')\n",
    "#     model.fit(pool,eval_set=val_pool,use_best_model=True)\n",
    "#     val_pr[val_index]=model.predict(val_x)\n",
    "#     test_pr+=model.predict(test_x,prediction_type='Probability')[:,1]/num_folds\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# accuracy_score(y, val_pr)\n",
    "\n",
    "\n",
    "val_pr=np.zeros(len(X))\n",
    "test_pr=np.zeros(len(test_x))\n",
    "for train_index,val_index in rskf.split(X,y):\n",
    "    train_x=X[train_index]\n",
    "    train_y=y[train_index]\n",
    "    val_x=X[val_index]\n",
    "    val_y=y[val_index]\n",
    "    pool=lgb.Dataset(train_x,label=train_y)\n",
    "    val_pool=lgb.Dataset(val_x,label=val_y)\n",
    "    num_round=10000\n",
    "    model = lgb.train(params,pool,num_round,valid_sets=[pool,val_pool],verbose_eval=100,early_stopping_rounds=400)\n",
    "    val_pr[val_index]=model.predict(val_x,num_iteration=model.best_iteration)\n",
    "    test_pr+=model.predict(test_x,num_iteration=model.best_iteration)/num_folds\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, np.where(val_pr>0.5,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | lambda_l1 | lambda_l2 | max_depth | min_child | min_ch... | min_sp... | n_esti... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9395  \u001b[0m | \u001b[0m 187.2   \u001b[0m | \u001b[0m 9.326   \u001b[0m | \u001b[0m 10.26   \u001b[0m | \u001b[0m 87.62   \u001b[0m | \u001b[0m 17.17   \u001b[0m | \u001b[0m 0.07309 \u001b[0m | \u001b[0m 4.956e+0\u001b[0m | \u001b[0m 40.08   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.9395  \u001b[0m | \u001b[0m 42.86   \u001b[0m | \u001b[0m 126.0   \u001b[0m | \u001b[0m 2.716   \u001b[0m | \u001b[0m 129.3   \u001b[0m | \u001b[0m 21.78   \u001b[0m | \u001b[0m 0.03776 \u001b[0m | \u001b[0m 2.759e+0\u001b[0m | \u001b[0m 26.73   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.9395  \u001b[0m | \u001b[0m 43.11   \u001b[0m | \u001b[0m 85.44   \u001b[0m | \u001b[0m 11.48   \u001b[0m | \u001b[0m 126.5   \u001b[0m | \u001b[0m 23.76   \u001b[0m | \u001b[0m 0.02914 \u001b[0m | \u001b[0m 1.571e+0\u001b[0m | \u001b[0m 47.05   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.9395  \u001b[0m | \u001b[0m 182.9   \u001b[0m | \u001b[0m 0.5625  \u001b[0m | \u001b[0m 13.1    \u001b[0m | \u001b[0m 69.83   \u001b[0m | \u001b[0m 17.18   \u001b[0m | \u001b[0m 0.07653 \u001b[0m | \u001b[0m 2e+04   \u001b[0m | \u001b[0m 25.59   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.9395  \u001b[0m | \u001b[0m 191.7   \u001b[0m | \u001b[0m 4.728   \u001b[0m | \u001b[0m 8.606   \u001b[0m | \u001b[0m 70.45   \u001b[0m | \u001b[0m 26.11   \u001b[0m | \u001b[0m 0.004621\u001b[0m | \u001b[0m 1.019e+0\u001b[0m | \u001b[0m 33.61   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.9395  \u001b[0m | \u001b[0m 199.3   \u001b[0m | \u001b[0m 196.8   \u001b[0m | \u001b[0m 15.3    \u001b[0m | \u001b[0m 17.08   \u001b[0m | \u001b[0m 25.55   \u001b[0m | \u001b[0m 0.08076 \u001b[0m | \u001b[0m 1.064e+0\u001b[0m | \u001b[0m 47.03   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.9801  \u001b[0m | \u001b[95m 1.87    \u001b[0m | \u001b[95m 4.944   \u001b[0m | \u001b[95m-0.3618  \u001b[0m | \u001b[95m 17.32   \u001b[0m | \u001b[95m 33.31   \u001b[0m | \u001b[95m 0.077   \u001b[0m | \u001b[95m 9.814e+0\u001b[0m | \u001b[95m 21.08   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.9797  \u001b[0m | \u001b[0m 0.992   \u001b[0m | \u001b[0m 6.801   \u001b[0m | \u001b[0m 0.6029  \u001b[0m | \u001b[0m 148.8   \u001b[0m | \u001b[0m 38.22   \u001b[0m | \u001b[0m 0.09326 \u001b[0m | \u001b[0m 8.083e+0\u001b[0m | \u001b[0m 28.24   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.9834  \u001b[0m | \u001b[95m 6.755   \u001b[0m | \u001b[95m 0.339   \u001b[0m | \u001b[95m 10.65   \u001b[0m | \u001b[95m 139.8   \u001b[0m | \u001b[95m 15.34   \u001b[0m | \u001b[95m 0.08092 \u001b[0m | \u001b[95m 1.135e+0\u001b[0m | \u001b[95m 40.03   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.9825  \u001b[0m | \u001b[0m 12.86   \u001b[0m | \u001b[0m 0.6395  \u001b[0m | \u001b[0m 19.76   \u001b[0m | \u001b[0m 148.7   \u001b[0m | \u001b[0m 14.68   \u001b[0m | \u001b[0m 0.036   \u001b[0m | \u001b[0m 9.547e+0\u001b[0m | \u001b[0m 44.16   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.9811  \u001b[0m | \u001b[0m 11.59   \u001b[0m | \u001b[0m 1.404   \u001b[0m | \u001b[0m 12.08   \u001b[0m | \u001b[0m 147.1   \u001b[0m | \u001b[0m 27.24   \u001b[0m | \u001b[0m 0.05192 \u001b[0m | \u001b[0m 1.046e+0\u001b[0m | \u001b[0m 39.62   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.9804  \u001b[0m | \u001b[0m 2.981   \u001b[0m | \u001b[0m 2.635   \u001b[0m | \u001b[0m 18.51   \u001b[0m | \u001b[0m 128.5   \u001b[0m | \u001b[0m 37.77   \u001b[0m | \u001b[0m 0.08823 \u001b[0m | \u001b[0m 1.226e+0\u001b[0m | \u001b[0m 27.8    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 13      \u001b[0m | \u001b[95m 0.9853  \u001b[0m | \u001b[95m 2.094   \u001b[0m | \u001b[95m 3.83    \u001b[0m | \u001b[95m-0.09846 \u001b[0m | \u001b[95m 27.37   \u001b[0m | \u001b[95m 7.076   \u001b[0m | \u001b[95m 0.06587 \u001b[0m | \u001b[95m 1.078e+0\u001b[0m | \u001b[95m 48.62   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.9848  \u001b[0m | \u001b[0m 0.2234  \u001b[0m | \u001b[0m 9.723   \u001b[0m | \u001b[0m 19.61   \u001b[0m | \u001b[0m 33.45   \u001b[0m | \u001b[0m 8.067   \u001b[0m | \u001b[0m 0.01609 \u001b[0m | \u001b[0m 8.409e+0\u001b[0m | \u001b[0m 38.66   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.9808  \u001b[0m | \u001b[0m 0.1126  \u001b[0m | \u001b[0m 7.085   \u001b[0m | \u001b[0m 16.52   \u001b[0m | \u001b[0m 11.38   \u001b[0m | \u001b[0m 36.87   \u001b[0m | \u001b[0m 0.07602 \u001b[0m | \u001b[0m 1.159e+0\u001b[0m | \u001b[0m 49.44   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.9848  \u001b[0m | \u001b[0m 4.283   \u001b[0m | \u001b[0m 3.836   \u001b[0m | \u001b[0m 3.559   \u001b[0m | \u001b[0m 60.28   \u001b[0m | \u001b[0m 6.856   \u001b[0m | \u001b[0m 0.0534  \u001b[0m | \u001b[0m 8.863e+0\u001b[0m | \u001b[0m 45.41   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 17      \u001b[0m | \u001b[95m 0.986   \u001b[0m | \u001b[95m 0.2585  \u001b[0m | \u001b[95m 1.469   \u001b[0m | \u001b[95m 14.5    \u001b[0m | \u001b[95m 72.51   \u001b[0m | \u001b[95m 6.389   \u001b[0m | \u001b[95m 0.0641  \u001b[0m | \u001b[95m 1.067e+0\u001b[0m | \u001b[95m 31.76   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.9841  \u001b[0m | \u001b[0m 5.069   \u001b[0m | \u001b[0m 0.1306  \u001b[0m | \u001b[0m 2.057   \u001b[0m | \u001b[0m 100.5   \u001b[0m | \u001b[0m 7.772   \u001b[0m | \u001b[0m 0.07047 \u001b[0m | \u001b[0m 1.228e+0\u001b[0m | \u001b[0m 49.3    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.9822  \u001b[0m | \u001b[0m 1.106   \u001b[0m | \u001b[0m 7.38    \u001b[0m | \u001b[0m 1.244   \u001b[0m | \u001b[0m 23.51   \u001b[0m | \u001b[0m 6.755   \u001b[0m | \u001b[0m 0.05194 \u001b[0m | \u001b[0m 8.394e+0\u001b[0m | \u001b[0m 25.9    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.986   \u001b[0m | \u001b[0m 1.572   \u001b[0m | \u001b[0m 2.007   \u001b[0m | \u001b[0m 5.318   \u001b[0m | \u001b[0m 73.15   \u001b[0m | \u001b[0m 7.36    \u001b[0m | \u001b[0m 0.04089 \u001b[0m | \u001b[0m 1.16e+04\u001b[0m | \u001b[0m 37.59   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.9853  \u001b[0m | \u001b[0m 1.98    \u001b[0m | \u001b[0m 2.598   \u001b[0m | \u001b[0m 3.631   \u001b[0m | \u001b[0m 39.53   \u001b[0m | \u001b[0m 7.061   \u001b[0m | \u001b[0m 0.01871 \u001b[0m | \u001b[0m 1.214e+0\u001b[0m | \u001b[0m 33.92   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.9843  \u001b[0m | \u001b[0m 4.591   \u001b[0m | \u001b[0m 1.252   \u001b[0m | \u001b[0m 16.19   \u001b[0m | \u001b[0m 117.8   \u001b[0m | \u001b[0m 7.404   \u001b[0m | \u001b[0m 0.08416 \u001b[0m | \u001b[0m 1.098e+0\u001b[0m | \u001b[0m 24.24   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.9855  \u001b[0m | \u001b[0m 3.719   \u001b[0m | \u001b[0m 1.161   \u001b[0m | \u001b[0m 15.7    \u001b[0m | \u001b[0m 128.9   \u001b[0m | \u001b[0m 7.135   \u001b[0m | \u001b[0m 0.03091 \u001b[0m | \u001b[0m 8.245e+0\u001b[0m | \u001b[0m 47.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.985   \u001b[0m | \u001b[0m 4.208   \u001b[0m | \u001b[0m 3.788   \u001b[0m | \u001b[0m 7.171   \u001b[0m | \u001b[0m 127.5   \u001b[0m | \u001b[0m 5.76    \u001b[0m | \u001b[0m 0.0141  \u001b[0m | \u001b[0m 1.151e+0\u001b[0m | \u001b[0m 46.63   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.985   \u001b[0m | \u001b[0m 3.305   \u001b[0m | \u001b[0m 1.652   \u001b[0m | \u001b[0m 17.76   \u001b[0m | \u001b[0m 132.5   \u001b[0m | \u001b[0m 6.156   \u001b[0m | \u001b[0m 0.06014 \u001b[0m | \u001b[0m 1.048e+0\u001b[0m | \u001b[0m 25.43   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.9848  \u001b[0m | \u001b[0m 3.013   \u001b[0m | \u001b[0m 0.5298  \u001b[0m | \u001b[0m 4.914   \u001b[0m | \u001b[0m 117.8   \u001b[0m | \u001b[0m 6.711   \u001b[0m | \u001b[0m 0.007775\u001b[0m | \u001b[0m 9.095e+0\u001b[0m | \u001b[0m 49.97   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.9846  \u001b[0m | \u001b[0m 3.848   \u001b[0m | \u001b[0m 2.464   \u001b[0m | \u001b[0m 19.56   \u001b[0m | \u001b[0m 20.35   \u001b[0m | \u001b[0m 8.717   \u001b[0m | \u001b[0m 0.06725 \u001b[0m | \u001b[0m 1.254e+0\u001b[0m | \u001b[0m 37.58   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (5.279388119672301, 1.3431495619072864, 10.593294744935063, 131.4767737974385, 9.237841114361787, 0.08249489867393767, 7091.757022184412, 45.131645927178134)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-418f0e14bbac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     50\u001b[0m optimizer.maximize(init_points=3,\n\u001b[1;32m---> 51\u001b[1;33m     n_iter=50)\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTMIZATION_END\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTMIZATION_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-418f0e14bbac>\u001b[0m in \u001b[0;36moptim_lgb\u001b[1;34m(min_child, n_estimator, max_depth, num_leaves, lambda_l1, lambda_l2, min_split_gain, min_child_weight)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mval_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mval_pr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_lgb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_child\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_leaves\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambda_l1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambda_l2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_split_gain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_child_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mthresholds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mthresh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-418f0e14bbac>\u001b[0m in \u001b[0;36mrun_lgb\u001b[1;34m(train_data, val_data, val_x, min_child, n_estimator, max_depth, num_leaves, lambda_l1, lambda_l2, min_split_gain, min_child_weight)\u001b[0m\n\u001b[0;32m     18\u001b[0m                      \u001b[1;34m'min_child_weight'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmin_child_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 }\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mpred_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    214\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   1758\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   1759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1760\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   1761\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1762\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def optim_lgb(min_child,n_estimator,max_depth,num_leaves,lambda_l1,lambda_l2,min_split_gain,min_child_weight):\n",
    "    def run_lgb(train_data,val_data,val_x,min_child,n_estimator,max_depth,num_leaves,lambda_l1,lambda_l2,min_split_gain,min_child_weight):\n",
    "        num_round = 10000\n",
    "        params = {\n",
    "                    \"objective\" : \"binary\",\n",
    "                    \"metric\" : \"binary_error\", \n",
    "                    \"boosting\": \"gbdt\",\n",
    "                    'n_jobs': 6,\n",
    "                   'learning_rate': 0.008,\n",
    "                     'min_child_samples': int(min_child),\n",
    "                    'cat_smooth': 10.0,\n",
    "                    'n_estimators': int(n_estimator),\n",
    "                     'max_depth':int(round(max_depth)),\n",
    "                     'num_leaves':int(num_leaves),\n",
    "                     'lambda_l1':max(lambda_l1, 0),\n",
    "                     'lambda_l2':max(lambda_l2, 0),\n",
    "                     'min_split_gain':min_split_gain,\n",
    "                     'min_child_weight':min_child_weight\n",
    "                }\n",
    "        model=lgb.train(params,train_data,num_round,valid_sets=[train_data,val_data],verbose_eval=False,early_stopping_rounds=400)\n",
    "\n",
    "        pred_val = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "    #    return pred_val\n",
    "        return  pred_val\n",
    "    rskf=StratifiedKFold(5,shuffle=True,random_state=315)\n",
    "    val_pr=np.zeros(len(X))\n",
    "    for train_index,val_index in rskf.split(X,y):\n",
    "        train_x=X[train_index]\n",
    "        train_y=y[train_index]\n",
    "        val_x=X[val_index]\n",
    "        val_y=y[val_index]\n",
    "        train_data=lgb.Dataset(train_x,label=train_y)\n",
    "        val_data=lgb.Dataset(val_x,label=val_y)\n",
    "        val_pr[val_index]=run_lgb(train_data,val_data,val_x,min_child,n_estimator,max_depth,num_leaves,lambda_l1,lambda_l2,min_split_gain,min_child_weight)\n",
    "    thresholds = []\n",
    "    for thresh in np.arange(0.01, 1, 0.01):\n",
    "        thresh = np.round(thresh, 2)\n",
    "        pr=np.where(val_pr>thresh,1,0)\n",
    "        res = accuracy_score(y, pr)\n",
    "        thresholds.append([thresh, res])\n",
    "    thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "    best_thresh = thresholds[0][0]\n",
    "    return accuracy_score(y, np.where(val_pr>best_thresh,1,0))\n",
    "    \n",
    "from bayes_opt import BayesianOptimization\n",
    "optimizer = BayesianOptimization(optim_lgb,{'min_child':(10,150),'n_estimator':(1000,20000),'max_depth':(-1,20),'num_leaves':(20,50),'lambda_l1':(0,200),'lambda_l2':(0,200),\n",
    "'min_split_gain':(0.001, 0.1),'min_child_weight':(5,40)}\n",
    "    \n",
    ")\n",
    "optimizer.maximize(init_points=3,\n",
    "    n_iter=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Загружено 1.2891581797086502 % фотографий\n",
      "Загружено 2.5783163594173004 % фотографий\n",
      "Загружено 3.8674745391259506 % фотографий\n",
      "Загружено 5.156632718834601 % фотографий\n",
      "Загружено 6.445790898543251 % фотографий\n",
      "Загружено 7.734949078251901 % фотографий\n",
      "Загружено 9.024107257960551 % фотографий\n",
      "Загружено 10.313265437669202 % фотографий\n",
      "Загружено 11.602423617377852 % фотографий\n",
      "Загружено 12.891581797086502 % фотографий\n",
      "Загружено 14.180739976795154 % фотографий\n",
      "Загружено 15.469898156503803 % фотографий\n",
      "Загружено 16.759056336212453 % фотографий\n",
      "Загружено 18.048214515921103 % фотографий\n",
      "Загружено 19.337372695629753 % фотографий\n",
      "Загружено 20.626530875338403 % фотографий\n",
      "Загружено 21.915689055047054 % фотографий\n",
      "Загружено 23.204847234755704 % фотографий\n",
      "Загружено 24.494005414464354 % фотографий\n",
      "Загружено 25.783163594173004 % фотографий\n",
      "Загружено 27.072321773881658 % фотографий\n",
      "Загружено 28.361479953590308 % фотографий\n",
      "Загружено 29.650638133298955 % фотографий\n",
      "Загружено 30.939796313007605 % фотографий\n",
      "Загружено 32.228954492716255 % фотографий\n",
      "Загружено 33.518112672424905 % фотографий\n",
      "Загружено 34.80727085213356 % фотографий\n",
      "Загружено 36.096429031842206 % фотографий\n",
      "Загружено 37.385587211550856 % фотографий\n",
      "Загружено 38.674745391259506 % фотографий\n",
      "Загружено 39.96390357096816 % фотографий\n",
      "Загружено 41.25306175067681 % фотографий\n",
      "Загружено 42.542219930385464 % фотографий\n",
      "Загружено 43.83137811009411 % фотографий\n",
      "Загружено 45.12053628980276 % фотографий\n",
      "Загружено 46.40969446951141 % фотографий\n",
      "Загружено 47.69885264922006 % фотографий\n",
      "Загружено 48.98801082892871 % фотографий\n",
      "Загружено 50.27716900863736 % фотографий\n",
      "Загружено 51.56632718834601 % фотографий\n",
      "Загружено 52.85548536805466 % фотографий\n",
      "Загружено 54.144643547763316 % фотографий\n",
      "Загружено 55.433801727471966 % фотографий\n",
      "Загружено 56.722959907180616 % фотографий\n",
      "Загружено 58.01211808688927 % фотографий\n",
      "Загружено 59.30127626659791 % фотографий\n",
      "Загружено 60.59043444630656 % фотографий\n",
      "Загружено 61.87959262601521 % фотографий\n",
      "Загружено 63.16875080572386 % фотографий\n",
      "Загружено 64.45790898543251 % фотографий\n",
      "Загружено 65.74706716514116 % фотографий\n",
      "Загружено 67.03622534484981 % фотографий\n",
      "Загружено 68.32538352455846 % фотографий\n",
      "Загружено 69.61454170426713 % фотографий\n",
      "Загружено 70.90369988397576 % фотографий\n",
      "Загружено 72.19285806368441 % фотографий\n",
      "Загружено 73.48201624339306 % фотографий\n",
      "Загружено 74.77117442310171 % фотографий\n",
      "Загружено 76.06033260281036 % фотографий\n",
      "Загружено 77.34949078251901 % фотографий\n",
      "Загружено 78.63864896222766 % фотографий\n",
      "Загружено 79.92780714193631 % фотографий\n",
      "Загружено 81.21696532164496 % фотографий\n",
      "Загружено 82.50612350135361 % фотографий\n",
      "Загружено 83.79528168106226 % фотографий\n",
      "Загружено 85.08443986077093 % фотографий\n",
      "Загружено 86.37359804047958 % фотографий\n",
      "Загружено 87.66275622018821 % фотографий\n",
      "Загружено 88.95191439989686 % фотографий\n",
      "Загружено 90.24107257960551 % фотографий\n",
      "Загружено 91.53023075931416 % фотографий\n",
      "Загружено 92.81938893902282 % фотографий\n",
      "Загружено 94.10854711873147 % фотографий\n",
      "Загружено 95.39770529844012 % фотографий\n",
      "Загружено 96.68686347814877 % фотографий\n",
      "Загружено 97.97602165785742 % фотографий\n",
      "Загружено 99.26517983756607 % фотографий\n",
      "Загружено 82.64462809917356 % фотографий\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_error: 0.0241741\tvalid_1's binary_error: 0.0483247\n",
      "[200]\ttraining's binary_error: 0.0143433\tvalid_1's binary_error: 0.0354381\n",
      "[300]\ttraining's binary_error: 0.00902498\tvalid_1's binary_error: 0.0302835\n",
      "[400]\ttraining's binary_error: 0.00531829\tvalid_1's binary_error: 0.0257732\n",
      "[500]\ttraining's binary_error: 0.00241741\tvalid_1's binary_error: 0.0186856\n",
      "[600]\ttraining's binary_error: 0.00209508\tvalid_1's binary_error: 0.0154639\n",
      "[700]\ttraining's binary_error: 0.00145044\tvalid_1's binary_error: 0.0115979\n",
      "[800]\ttraining's binary_error: 0.00145044\tvalid_1's binary_error: 0.0109536\n",
      "[900]\ttraining's binary_error: 0.00145044\tvalid_1's binary_error: 0.0103093\n",
      "[1000]\ttraining's binary_error: 0.00145044\tvalid_1's binary_error: 0.0103093\n",
      "Early stopping, best iteration is:\n",
      "[634]\ttraining's binary_error: 0.00145044\tvalid_1's binary_error: 0.0128866\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_error: 0.0246575\tvalid_1's binary_error: 0.0360825\n",
      "[200]\ttraining's binary_error: 0.0151491\tvalid_1's binary_error: 0.0277062\n",
      "[300]\ttraining's binary_error: 0.0093473\tvalid_1's binary_error: 0.0219072\n",
      "[400]\ttraining's binary_error: 0.00451249\tvalid_1's binary_error: 0.0135309\n",
      "[500]\ttraining's binary_error: 0.00273973\tvalid_1's binary_error: 0.0141753\n",
      "[600]\ttraining's binary_error: 0.00193392\tvalid_1's binary_error: 0.00902062\n",
      "[700]\ttraining's binary_error: 0.00193392\tvalid_1's binary_error: 0.00966495\n",
      "[800]\ttraining's binary_error: 0.00193392\tvalid_1's binary_error: 0.00902062\n",
      "[900]\ttraining's binary_error: 0.00193392\tvalid_1's binary_error: 0.00902062\n",
      "Early stopping, best iteration is:\n",
      "[564]\ttraining's binary_error: 0.00193392\tvalid_1's binary_error: 0.00966495\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_error: 0.0227199\tvalid_1's binary_error: 0.0322373\n",
      "[200]\ttraining's binary_error: 0.0149855\tvalid_1's binary_error: 0.0296583\n",
      "[300]\ttraining's binary_error: 0.0106349\tvalid_1's binary_error: 0.0238556\n",
      "[400]\ttraining's binary_error: 0.00499517\tvalid_1's binary_error: 0.0186976\n",
      "[500]\ttraining's binary_error: 0.00306155\tvalid_1's binary_error: 0.0167634\n",
      "[600]\ttraining's binary_error: 0.00209475\tvalid_1's binary_error: 0.0122502\n",
      "[700]\ttraining's binary_error: 0.00193361\tvalid_1's binary_error: 0.0116054\n",
      "[800]\ttraining's binary_error: 0.00193361\tvalid_1's binary_error: 0.0116054\n",
      "[900]\ttraining's binary_error: 0.00193361\tvalid_1's binary_error: 0.0109607\n",
      "[1000]\ttraining's binary_error: 0.00193361\tvalid_1's binary_error: 0.00967118\n",
      "Early stopping, best iteration is:\n",
      "[685]\ttraining's binary_error: 0.00193361\tvalid_1's binary_error: 0.0116054\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_error: 0.0227199\tvalid_1's binary_error: 0.03804\n",
      "[200]\ttraining's binary_error: 0.0140187\tvalid_1's binary_error: 0.0283688\n",
      "[300]\ttraining's binary_error: 0.00854012\tvalid_1's binary_error: 0.0212766\n",
      "[400]\ttraining's binary_error: 0.00531743\tvalid_1's binary_error: 0.0193424\n",
      "[500]\ttraining's binary_error: 0.00257815\tvalid_1's binary_error: 0.0148291\n",
      "[600]\ttraining's binary_error: 0.00177248\tvalid_1's binary_error: 0.0122502\n",
      "[700]\ttraining's binary_error: 0.00161134\tvalid_1's binary_error: 0.0103159\n",
      "[800]\ttraining's binary_error: 0.00161134\tvalid_1's binary_error: 0.00902643\n",
      "[900]\ttraining's binary_error: 0.00161134\tvalid_1's binary_error: 0.00902643\n",
      "[1000]\ttraining's binary_error: 0.00161134\tvalid_1's binary_error: 0.00902643\n",
      "Early stopping, best iteration is:\n",
      "[608]\ttraining's binary_error: 0.00161134\tvalid_1's binary_error: 0.0109607\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_error: 0.0232034\tvalid_1's binary_error: 0.0367505\n",
      "[200]\ttraining's binary_error: 0.0157912\tvalid_1's binary_error: 0.0257898\n",
      "[300]\ttraining's binary_error: 0.0098292\tvalid_1's binary_error: 0.0206319\n",
      "[400]\ttraining's binary_error: 0.00435063\tvalid_1's binary_error: 0.0148291\n",
      "[500]\ttraining's binary_error: 0.00209475\tvalid_1's binary_error: 0.0128949\n",
      "[600]\ttraining's binary_error: 0.00177248\tvalid_1's binary_error: 0.0103159\n",
      "[700]\ttraining's binary_error: 0.00145021\tvalid_1's binary_error: 0.00644745\n",
      "[800]\ttraining's binary_error: 0.00145021\tvalid_1's binary_error: 0.00580271\n",
      "[900]\ttraining's binary_error: 0.00145021\tvalid_1's binary_error: 0.00838169\n",
      "[1000]\ttraining's binary_error: 0.00145021\tvalid_1's binary_error: 0.0070922\n",
      "Early stopping, best iteration is:\n",
      "[641]\ttraining's binary_error: 0.00145021\tvalid_1's binary_error: 0.00644745\n",
      "Wall time: 2h 48min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#model and gray balance in array\n",
    "from PIL import Image,ImageFile\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.applications.nasnet import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.nasnet import NASNetLarge\n",
    "from keras.applications.nasnet import preprocess_input\n",
    "from keras.applications.nasnet import decode_predictions\n",
    "model=NASNetLarge()\n",
    "def class_prediction(img_pil,model=model):\n",
    "    img_pil=np.expand_dims(img_pil,axis=0)\n",
    "    features=model.predict(img_pil)\n",
    "    result=decode_predictions(features)\n",
    "    result=[result[0][0][1],result[0][1][1],result[0][2][1],result[0][3][1]]\n",
    "    return result\n",
    "def parse_label(path):\n",
    "    data=pd.DataFrame(columns=['name','label'])\n",
    "    for folder in os.listdir(path):\n",
    "        for file in os.listdir(path+\"\\\\\"+folder):\n",
    "            data.loc[len(data)]=[file,folder]\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    return data\n",
    "train_data=parse_label('d:\\Python\\cv\\\\model_data')\n",
    "test_data=parse_label('d:\\Python\\cv\\\\test_data')\n",
    "# def prepare_x(data,path):\n",
    "#     X=np.zeros((len(data),257))\n",
    "#     count=0\n",
    "#     stopwords=['envelope','menu','web_site','slide_rule']\n",
    "#     for fig,label in zip(data['name'].values,data['label'].values):\n",
    "#         img=image.load_img(path+'\\\\'+label+'\\\\'+fig,target_size=(331,331,3))\n",
    "#         img=image.img_to_array(img)\n",
    "#         num=np.squeeze(cv2.calcHist([img],[0],None,[256],[0,256]))\n",
    "#         img=preprocess_input(img)\n",
    "#         cat=class_prediction(img)\n",
    "#         X[count]=np.insert(num,0,sum([1 if word in cat else 0 for word in stopwords]))\n",
    "#         count+=1\n",
    "#         print('Загружено %s %% фотографий'%count/len(data))\n",
    "#     return X\n",
    "def prepare_x(data,path):\n",
    "    X=np.zeros((len(data),260))\n",
    "    count=0\n",
    "    stopwords=['envelope','menu','web_site','slide_rule']\n",
    "    for fig,label in zip(data['name'].values,data['label'].values):\n",
    "        img=image.load_img(path+'\\\\'+label+'\\\\'+fig,target_size=(331,331,3))\n",
    "        img=image.img_to_array(img)\n",
    "        num=np.squeeze(cv2.calcHist([img],[0],None,[256],[0,256]))\n",
    "        img=preprocess_input(img)\n",
    "        cat=class_prediction(img)\n",
    "        X[count]=np.concatenate((num,[1 if word in stopwords else 0 for word in cat]))\n",
    "        count+=1\n",
    "        if count%100==0:\n",
    "            print('Загружено %s %% фотографий'%((count/len(data))*100))\n",
    "    return X\n",
    "train_x=prepare_x(train_data,'d:\\Python\\cv\\\\model_data')\n",
    "test_x=prepare_x(test_data,'d:\\Python\\cv\\\\test_data')\n",
    "train_data['label']=train_data['label'].replace({'pictures_bad':1,'pictures_good':0})\n",
    "test_data['label']=test_data['label'].replace({'pictures_bad':1,'pictures_good':0})\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "num_folds=5\n",
    "params={'n_estimators': 6100, \n",
    "             'n_jobs': 6,\n",
    "         'objective':'binary',\n",
    "         'max_depth': 15,\n",
    "         'learning_rate': 0.01,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"metric\": 'binary_error',\n",
    "         \"random_state\": 2333\n",
    "    }\n",
    "rskf=StratifiedKFold(num_folds,shuffle=True)\n",
    "val_pr=np.zeros(len(train_x))\n",
    "test_pr=np.zeros(len(test_x))\n",
    "X=train_x.copy()\n",
    "y=train_data['label']\n",
    "for train_index,val_index in rskf.split(X,y):\n",
    "    train_x=X[train_index]\n",
    "    train_y=y[train_index]\n",
    "    val_x=X[val_index]\n",
    "    val_y=y[val_index]\n",
    "    pool=lgb.Dataset(train_x,label=train_y)\n",
    "    val_pool=lgb.Dataset(val_x,label=val_y)\n",
    "    num_round=10000\n",
    "    model = lgb.train(params,pool,num_round,valid_sets=[pool,val_pool],verbose_eval=100,early_stopping_rounds=400)\n",
    "    val_pr[val_index]=model.predict(val_x,num_iteration=model.best_iteration)\n",
    "    test_pr+=model.predict(test_x,num_iteration=model.best_iteration)/num_folds\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, np.where(val_pr>0.5,1,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image,ImageFile\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.applications.nasnet import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.nasnet import NASNetLarge\n",
    "from keras.applications.nasnet import preprocess_input\n",
    "from keras.applications.nasnet import decode_predictions\n",
    "model=NASNetLarge()\n",
    "def class_prediction(img_pil,model=model):\n",
    "    img_pil=np.expand_dims(img_pil,axis=0)\n",
    "    features=model.predict(img_pil)\n",
    "    result=decode_predictions(features)\n",
    "    result=[result[0][0][1],result[0][1][1],result[0][2][1],result[0][3][1],result[0][4][1]]\n",
    "    return result\n",
    "def parse_label(path):\n",
    "    data=pd.DataFrame(columns=['name','label'])\n",
    "    for folder in os.listdir(path):\n",
    "        for file in os.listdir(path+\"\\\\\"+folder):\n",
    "            data.loc[len(data)]=[file,folder]\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    return data\n",
    "train_data=parse_label('d:\\Python\\cv\\\\model_data')\n",
    "test_data=parse_label('d:\\Python\\cv\\\\test_data')\n",
    "def prepare_x(data,path):\n",
    "    X_cat=pd.DataFrame(columns=['feature1','feature2','feature3','feature4','feature5',])\n",
    "    X_num=np.zeros((len(data),256))\n",
    "    count=0\n",
    "    for fig,label in zip(data['name'].values,data['label'].values):\n",
    "        img=image.load_img(path+'\\\\'+label+'\\\\'+fig,target_size=(331,331,3))\n",
    "        img=image.img_to_array(img)\n",
    "        X_num[count]=np.squeeze(cv2.calcHist([img],[0],None,[256],[0,256]))\n",
    "        img=preprocess_input(img)\n",
    "        X_cat.loc[count]=class_prediction(img)\n",
    "        count+=1\n",
    "    return X_cat,X_num\n",
    "\n",
    "train_cat,train_num=prepare_x(train_data,'d:\\Python\\cv\\\\model_data')\n",
    "test_cat,test_num=prepare_x(test_data,'d:\\Python\\cv\\\\test_data')\n",
    "train_data['label']=train_data['label'].replace({'pictures_bad':1,'pictures_good':0})\n",
    "test_data['label']=test_data['label'].replace({'pictures_bad':1,'pictures_good':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9668267\ttest: 0.9856021\tbest: 0.9856021 (0)\ttotal: 25.7ms\tremaining: 7.68s\n",
      "100:\tlearn: 0.9903972\ttest: 0.9869110\tbest: 0.9869110 (100)\ttotal: 1.89s\tremaining: 3.73s\n",
      "200:\tlearn: 0.9947621\ttest: 0.9869110\tbest: 0.9869110 (100)\ttotal: 3.75s\tremaining: 1.85s\n",
      "299:\tlearn: 0.9960716\ttest: 0.9869110\tbest: 0.9869110 (100)\ttotal: 5.61s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9869109948\n",
      "bestIteration = 100\n",
      "\n",
      "Shrink model to first 101 iterations.\n",
      "0:\tlearn: 0.9716281\ttest: 0.9725131\tbest: 0.9725131 (0)\ttotal: 15.8ms\tremaining: 4.73s\n",
      "100:\tlearn: 0.9886512\ttest: 0.9751309\tbest: 0.9751309 (100)\ttotal: 1.73s\tremaining: 3.41s\n",
      "200:\tlearn: 0.9965081\ttest: 0.9764398\tbest: 0.9764398 (200)\ttotal: 3.74s\tremaining: 1.84s\n",
      "299:\tlearn: 0.9991270\ttest: 0.9764398\tbest: 0.9764398 (200)\ttotal: 5.7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9764397906\n",
      "bestIteration = 200\n",
      "\n",
      "Shrink model to first 201 iterations.\n",
      "0:\tlearn: 0.9703186\ttest: 0.9698953\tbest: 0.9698953 (0)\ttotal: 18.3ms\tremaining: 5.48s\n",
      "100:\tlearn: 0.9921432\ttest: 0.9698953\tbest: 0.9698953 (0)\ttotal: 1.91s\tremaining: 3.77s\n",
      "200:\tlearn: 0.9973811\ttest: 0.9685864\tbest: 0.9698953 (0)\ttotal: 3.85s\tremaining: 1.9s\n",
      "299:\tlearn: 0.9991270\ttest: 0.9698953\tbest: 0.9698953 (0)\ttotal: 5.95s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.969895288\n",
      "bestIteration = 0\n",
      "\n",
      "Shrink model to first 1 iterations.\n",
      "0:\tlearn: 0.9659686\ttest: 0.9829620\tbest: 0.9829620 (0)\ttotal: 16.2ms\tremaining: 4.83s\n",
      "100:\tlearn: 0.9873473\ttest: 0.9868938\tbest: 0.9868938 (100)\ttotal: 1.8s\tremaining: 3.54s\n",
      "200:\tlearn: 0.9960733\ttest: 0.9882045\tbest: 0.9882045 (200)\ttotal: 3.84s\tremaining: 1.89s\n",
      "299:\tlearn: 0.9982548\ttest: 0.9908257\tbest: 0.9908257 (299)\ttotal: 5.88s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9908256881\n",
      "bestIteration = 299\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_error: 0.0266259\tvalid_1's binary_error: 0.0379581\n",
      "[200]\ttraining's binary_error: 0.0100393\tvalid_1's binary_error: 0.0353403\n",
      "[300]\ttraining's binary_error: 0.000872981\tvalid_1's binary_error: 0.0353403\n",
      "[400]\ttraining's binary_error: 0.000872981\tvalid_1's binary_error: 0.0340314\n",
      "[500]\ttraining's binary_error: 0.000872981\tvalid_1's binary_error: 0.0340314\n",
      "[600]\ttraining's binary_error: 0.000872981\tvalid_1's binary_error: 0.0340314\n",
      "Early stopping, best iteration is:\n",
      "[298]\ttraining's binary_error: 0.000872981\tvalid_1's binary_error: 0.0353403\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_error: 0.0270624\tvalid_1's binary_error: 0.0379581\n",
      "[200]\ttraining's binary_error: 0.00742034\tvalid_1's binary_error: 0.0340314\n",
      "[300]\ttraining's binary_error: 0.00130947\tvalid_1's binary_error: 0.0314136\n",
      "[400]\ttraining's binary_error: 0.000872981\tvalid_1's binary_error: 0.0327225\n",
      "[500]\ttraining's binary_error: 0.000872981\tvalid_1's binary_error: 0.0301047\n",
      "[600]\ttraining's binary_error: 0.000872981\tvalid_1's binary_error: 0.0248691\n",
      "[700]\ttraining's binary_error: 0.000872981\tvalid_1's binary_error: 0.026178\n",
      "Early stopping, best iteration is:\n",
      "[301]\ttraining's binary_error: 0.000872981\tvalid_1's binary_error: 0.0314136\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_error: 0.0261894\tvalid_1's binary_error: 0.0497382\n",
      "[200]\ttraining's binary_error: 0.00742034\tvalid_1's binary_error: 0.0418848\n",
      "[300]\ttraining's binary_error: 0\tvalid_1's binary_error: 0.0379581\n",
      "[400]\ttraining's binary_error: 0\tvalid_1's binary_error: 0.039267\n",
      "[500]\ttraining's binary_error: 0\tvalid_1's binary_error: 0.0379581\n",
      "[600]\ttraining's binary_error: 0\tvalid_1's binary_error: 0.0379581\n",
      "Early stopping, best iteration is:\n",
      "[297]\ttraining's binary_error: 0\tvalid_1's binary_error: 0.0379581\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[100]\ttraining's binary_error: 0.0257417\tvalid_1's binary_error: 0.0366972\n",
      "[200]\ttraining's binary_error: 0.008726\tvalid_1's binary_error: 0.0393185\n",
      "[300]\ttraining's binary_error: 0.0017452\tvalid_1's binary_error: 0.0353866\n",
      "[400]\ttraining's binary_error: 0.0008726\tvalid_1's binary_error: 0.034076\n",
      "[500]\ttraining's binary_error: 0.0008726\tvalid_1's binary_error: 0.0314548\n",
      "[600]\ttraining's binary_error: 0.0008726\tvalid_1's binary_error: 0.0301442\n",
      "[700]\ttraining's binary_error: 0.0008726\tvalid_1's binary_error: 0.0301442\n",
      "Early stopping, best iteration is:\n",
      "[329]\ttraining's binary_error: 0.0008726\tvalid_1's binary_error: 0.0353866\n",
      "validation_accuracy:0.981342062193126\n",
      "test_accuracy:0.9669421487603306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#accuracy_score(test_data['label'], np.where(test_pr>0.5,1,0))\n",
    "#test_data[test_data['label']!=model.predict(test_x)]\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from catboost import CatBoostClassifier,Pool\n",
    "\n",
    "#Catboost\n",
    "num_folds=4\n",
    "params={'n_estimators': 6100, \n",
    "             'n_jobs': 6,\n",
    "         'objective':'binary',\n",
    "         'max_depth': 15,\n",
    "         'learning_rate': 0.01,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"metric\": 'binary_error',\n",
    "         \"random_state\": 2333\n",
    "    }\n",
    "rskf=StratifiedKFold(num_folds,shuffle=True)\n",
    "val_pr=np.zeros(len(train_cat))\n",
    "test_pr=np.zeros(len(test_cat))\n",
    "X=train_cat\n",
    "y=train_data['label']\n",
    "for train_index,val_index in rskf.split(X,y):\n",
    "    train_x=X.loc[train_index]\n",
    "    train_y=y[train_index]\n",
    "    val_x=X.loc[val_index]\n",
    "    val_y=y[val_index]\n",
    "    cat_features = [0,1,2,3,4]\n",
    "    pool=Pool(train_x,train_y,cat_features=cat_features)\n",
    "    val_pool=Pool(val_x,val_y,cat_features=cat_features)\n",
    "    model = CatBoostClassifier(iterations=300, metric_period=100,loss_function='CrossEntropy', eval_metric='Accuracy')\n",
    "    model.fit(pool,eval_set=val_pool,use_best_model=True)\n",
    "    val_pr[val_index]=model.predict(val_x,prediction_type='Probability')[:,1]\n",
    "    test_pr+=model.predict(test_cat,prediction_type='Probability')[:,1]/num_folds\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "result_train=pd.DataFrame({'target':y,'cat_prob':val_pr})\n",
    "result_test=pd.DataFrame({'target':test_data['label'],'cat_prob':test_pr})\n",
    "\n",
    "#LightGBM\n",
    "val_pr=np.zeros(len(train_num))\n",
    "test_pr=np.zeros(len(test_num))\n",
    "X=pd.DataFrame(train_num)\n",
    "y=train_data['label']\n",
    "for train_index,val_index in rskf.split(X,y):\n",
    "    train_x=X.loc[train_index]\n",
    "    train_y=y[train_index]\n",
    "    val_x=X.loc[val_index]\n",
    "    val_y=y[val_index]\n",
    "    pool=lgb.Dataset(train_x,label=train_y)\n",
    "    val_pool=lgb.Dataset(val_x,label=val_y)\n",
    "    num_round=10000\n",
    "    model = lgb.train(params,pool,num_round,valid_sets=[pool,val_pool],verbose_eval=100,early_stopping_rounds=400)\n",
    "    val_pr[val_index]=model.predict(val_x,num_iteration=model.best_iteration)\n",
    "    test_pr+=model.predict(test_num,num_iteration=model.best_iteration)/num_folds\n",
    "result_train['num_prob']=val_pr\n",
    "result_test['num_prob']=test_pr\n",
    "\n",
    "#LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "val_pr=np.zeros(len(result_train))\n",
    "test_pr=np.zeros(len(result_test))\n",
    "X=result_train[['num_prob','cat_prob']]\n",
    "y=result_train['target']\n",
    "for train_index,val_index in rskf.split(X,y):\n",
    "    train_x=X.loc[train_index]\n",
    "    train_y=y[train_index]\n",
    "    val_x=X.loc[val_index]\n",
    "    val_y=y[val_index]\n",
    "    model=LogisticRegression()\n",
    "    model.fit(train_x,train_y)\n",
    "    val_pr[val_index]=model.predict(val_x)\n",
    "    test_pr+=model.predict_proba(result_test[['num_prob','cat_prob']])[:,1]/num_folds\n",
    "print('validation_accuracy:%s\\ntest_accuracy:%s'%(accuracy_score(y, val_pr),accuracy_score(result_test['target'].values, np.where(test_pr>0.5,1,0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8760330578512396"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cat_val=val_pr.copy()\n",
    "#cat_test=test_pr.copy()\n",
    "#test_data[test_data['label']!=np.where(test_pr>0.5,1,0)]\n",
    "#accuracy_score(result_test['target'],np.where(result_test[['cat_prob','num_prob']].mean(axis=1)>0.5,1,0))\n",
    "#test_data[test_data['label']!=np.where(result_test[['cat_prob','num_prob']].mean(axis=1)>0.5,1,0)]\n",
    "#np.mean([cat_test,test_pr],axis=0)\n",
    "#accuracy_score(test_data['label'], np.where(np.mean([cat_test,test_pr],axis=0)>0.5,1,0))\n",
    "#test_data[test_data['label']!=np.where(test_pr>0.5,1,0)]\n",
    "#accuracy_score(y, np.where(val_pr>0.5,1,0))\n",
    "accuracy_score(test_data['label'], np.where(test_pr>0.5,1,0))\n",
    "#test_data[test_data['label']!=np.where(test_pr>0.5,1,0)]\n",
    "#train_data[train_data['label']!=np.where(val_pr>0.5,1,0)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
